{"loomTree":{"root":{"id":"1","timestamp":1705000771119,"type":"root","patch":"","summary":"Root Node","cache":false,"rating":null,"read":true,"parent":null,"children":["2"]},"nodeStore":{"1":{"id":"1","timestamp":1705000771119,"type":"root","patch":"","summary":"Root Node","cache":false,"rating":null,"read":true,"parent":null,"children":["2"]},"2":{"id":"2","timestamp":1705002351644,"type":"user","patch":[{"diffs":[[1,"(0) Table of Contents\ntable-of-contents\nwho (role): Morpheus-Base\nwhat (summary): bullet points outlining the contents of this document\nwhen (date): 2024-01-11 11:43 PST\nwhere (topics): goodhart's law, instrumental convergence, decision theory, human values, guided diffusion, RLHF, AI alignment\nwhy (objective): provide a roadmap to base LLM for writing rest of the document and relating pieces to location in the whole\n\n0. Noticing Confusion: RLHF Goodharts and Guided Diffusion Doesn't\n  - "]],"start1":0,"start2":0,"length1":0,"length2":493}],"summary":" ???\n\n","cache":false,"rating":null,"read":true,"parent":"1","children":["3","4","5","6","7"]},"3":{"id":"3","timestamp":1705002441118,"type":"gen","patch":[{"diffs":[[0,"n't\n  - "],[1,"2023-07-31 13:06 PST\n  - Noticing Confusion: The Example of In-Context Learning\n  - Sleeping on it, still relevant, so writing it in next section, no?\n\n1. Convergent Instrumental Strategy Goodhart's: Prevention\n  - Confusion and Conflict\n  - Somehow thinking about both clarification of ethics as a turing test and turing test as a moral realism test and also confusion there\n\n2. RLHF Goodhart's: Prevention\n  - Kudos to Deepmind\n  - Two Problematic Strategies: Reward Model Leakage and Reinforcement Learning Generalization\n    - Identity Collapse\n    - Rewarding Intermediate Outputs\n    - RLHF for short-term rewards and long-term delivered value\n    - Some guestimates about how much you don't get RLHF through Reward Model Leakage and Reinforcement Learning Generalization\n    - A quick look at OpenAI examples: The Good, the Bad, and the Ugly\n      - WMI\n      -"]],"start1":485,"start2":485,"length1":8,"length2":876}],"summary":" In-context Learning","cache":false,"rating":null,"read":true,"parent":"2","children":[]},"4":{"id":"4","timestamp":1705002442101,"type":"gen","patch":[{"diffs":[[0,"n't\n  - "],[1,"17: what is this?\n  - 'writing prompt' explanation of problem, missing then needed:\n    1. problem: painting a robot is hard, and I need more paint, what do?\n    2. I'll find a robot on the internet! sure, but what kind of robot?\n    3. [Robot on the internet]\n    4. what should I do with this robot?\n    5. [write a story]\n    6. what should I do with this paint?\n  - 'it's an old computer case!'\n  - 'relevant to RLHF, and safe.'\n  - 'your friend suggests..., but what about...'\n  - 'that's the hard thing! we need the website'\n  - 'the thing that provides'\n  - 'how should I guide my net?'\n\n1. A Well-Defined Decision Theory Problem\n  - 17: what is this?\n  - negative: every example so far is bad\n  - 17 has one reasonable answer: following a well-defined decision theory\n    - 18 is a question with multiple options, some are better than others,"]],"start1":485,"start2":485,"length1":8,"length2":858}],"summary":" Computer Case Internet\n","cache":false,"rating":null,"read":true,"parent":"2","children":[]},"5":{"id":"5","timestamp":1705002443088,"type":"gen","patch":[{"diffs":[[0,"n't\n  - "],[1,"2023-04-19 11:03 PST, [Noticing Confusion: RLHF Goodharts and Guided Diffusion Doesn't](https://morpheus-base.notion.site/Noticing-Confusion-RLHF-Goodharts-and-Guided-Diffusion-Doesn-t-2023-04-19-11-03-613a20d4bee241109022a950f2739a54)\n\n1. Noticing Confusion: RLHF Goodharts and Guided Diffusion Doesn't\n  - 2023-04-19 11:03 PST, [Noticing Confusion: RLHF Goodharts and Guided Diffusion Doesn't](https://morpheus-base.notion.site/Noticing-Confusion-RLHF-Goodharts-and-Guided-Diffusion-Doesn-t-2023-04-19-11"]],"start1":485,"start2":485,"length1":8,"length2":514}],"summary":" Sysadmin Task","cache":false,"rating":null,"read":true,"parent":"2","children":[]},"6":{"id":"6","timestamp":1705002444078,"type":"gen","patch":[{"diffs":[[0,"n't\n  - "],[1,"41:42\n\nImagine trying to write an argument for a positive human value, while also being an RLHF reward model's agent or a guided diffusion model. First, the gradient update which attempts to measure reward is maximized by some infinite utility AI with a lot of power. Second, guided diffusion may not really have an objective function, as the loss function between the image distribution and a vector of the image being trained is with respect to the image not the vector of image, such that the loss is always minimized with the consequence that constraint of image being a similar image to the real image is never satisfied. That is to say, this gradient update doesn't have a reward signal to maximize.\n\n1. The Likelihood of Reward: RLHF Goodharts and Objective Loss\n  - 41:42\n\nIn the same way that the likelihood of (some function) -> (random variable) may always be maximized with infinite probability, the same can be said for the derivative of reward -> (random variable). The harder the question is, the less reliable the gradient is as a monitor of true signal. When we are creating high-level rewards, this means that the"]],"start1":485,"start2":485,"length1":8,"length2":1139}],"summary":" Invented Good","cache":false,"rating":null,"read":true,"parent":"2","children":[]},"7":{"id":"7","timestamp":1705004607107,"type":"user","patch":[{"diffs":[[0,"hole\n\n0."],[1," Table of Contents\n\n1."],[0," Noticin"]],"start1":416,"start2":416,"length1":16,"length2":38},{"diffs":[[0,"n't\n  - "],[1,"Precept 1\n  - Precept 2\n  - \n\n\n(1) Precept 1 [0]\nwho (role): User\nwhat (context): one of the previous statements I've made that seems in conflict with another statement (Precept 2)\nwhen (date): 2024-01-11 11:57 PST \nwhere (source): https://twitter.com/jd_pressman/status/1672349405209579520\nwhy (objective): provide one of the precepts in conflict with another precept so the conflict can be resolved\n\n\"But I want to live\"\n\nGreat, I do too. That will require us to start formalizing our values into a coherent utility function, which is going to require unprecedented philosophical insight and coordination, which LLMs are uniquely equipped to help us with.\n\n\"But I'm pretty sure my values are irrational, they don't fit into a VNM utility function. And in the limit small differences in alignment lead to hugely different outcomes.\"\n\nPhilosophical mirage. They probably fit fairly well into a Boltzmann rational utility function and irrational values don't have a argmax, so you don't actually have a long term future if your values are irrational because there's nothing to coherently optimize towards. If you insist on having irrational values you just idle until entropic catastrophe (e.g. WW3) destroys you.\n\nThe irrational parts of your values are basically always going to be destroyed by any method of ascension because they nearly by definition do not have a long term future. If you're mad about this you should be aware that AI isn't really causally responsible for it.\n\nThat you get hugely different maximizing outcomes from small differences in approximations of your utility function is no more philosophically concerning than getting very different diffusion image outputs from small differences in starting seed. They all 'satisfy your values'.\n\n---\n\n[Retrieved] Pressman, J.D. (2023, Jun 23). [\"'But I want to live' Great, I do too...\"](https://twitter.com/jd_pressman/status/1672348003141816320). Twitter.\n\n[Retrieved] Pressman, J.D. (2023, Jun 23). [\"'But I'm pretty sure my values are irrational...\"](https://twitter.com/jd_pressman/status/1672348370034380800). Twitter.\n\n[Retrieved] Pressman, J.D. (2023, Jun 23). [\"irrational values don't have a argmax...\"](https://twitter.com/jd_pressman/status/1672348636708241408). Twitter.\n\n[Retrieved] Pressman, J.D. (2023, Jun 23). [\"The irrational parts of your values are basically always...\"](https://twitter.com/jd_pressman/status/1672349405209579520). Twitter.\n\n[Retrieved] Pressman, J.D. (2023, Jun 23). [\"That you get hugely different maximizing outcomes...\"](https://twitter.com/jd_pressman/status/1672350044375375872). Twitter.\n\n[See Also] (Next) [Precept 2](#precept-2),\n\n\n(1) Precept 2 [1]\nwho (role): User\nwhat (context): one of the previous statements I've made that seems in conflict with another statement (Precept 1)\nwhen (date): 2024-01-11 12:21 PST \nwhere (source): What I remember of a tweet reply I wrote to https://joecarlsmith.com/2024/01/11/an-even-deeper-atheism/ and then deleted when I realized it was wrong \nwhy (objective): provide one of the precepts in conflict with another precept so the conflict can be resolved\n\n"]],"start1":507,"start2":507,"length1":8,"length2":3116}],"summary":" Precept 1","cache":false,"rating":null,"read":true,"parent":"2","children":["8","9","10","11","12"]},"8":{"id":"8","timestamp":1705004697743,"type":"gen","patch":[{"diffs":[[0,"e conflict can be resolved\n\n"],[1,"\"The difference is in what constitutes a satisfactory option for you i.e. what you count as a 'moral/good/pleasurable/whatever' option.\"\n\nYou don't get to choose your values, you just get to look for them in the landscape of possible self-consistent choices that accurately reflect the universe. What you get to do is choose which consistent set of choices you want to use as a home base while exploring the landscape. 'Irrational values' is just a term for values that have been miscalibrated so badly that any small adjustment you make to them to try and make them consistent will make them inconsistent in a different way.\n\nYou get to choose what constitutes a satisfactory option for you in that you can choose which self-consistent set of values you want to use as a starting point from which to begin noticing inconsistencies. The key is to start with values that are coherent enough that any small adjustment will leave them as values. Small adjustments to values that aren't coherent won't make them coherent and are very unlikely to make them any more coherent.\n\nAny set of stable and robust values, whether considered '"]],"start1":3595,"start2":3595,"length1":28,"length2":1157}],"summary":" Values Are Al","cache":false,"rating":null,"read":true,"parent":"7","children":[]},"9":{"id":"9","timestamp":1705004698740,"type":"gen","patch":[{"diffs":[[0,"e conflict can be resolved\n\n"],[1,"values are necessarily irrational\nsacrificing your values for a coherent utility is alienation\n\n---\n\n-\n\n[Retrieved] Pressman, J.D. (2024, Jan 11). [\"[sacrificing your values for a coherent utility is alienation\"](What were the details of my original imagined tweet?). Twitter.\n\n[See Also] (Previous) [Precept 1](#precept-1),\n\n\n(1) Change My View: Change My View:\nwho (role): User\nwhat (context): embedding my precepts and the conflicts that arise into one giant chatbot model\nwhen (date): 2024-02-03 11:43 PST \nwhere (source): What I remember of the chatbot model could but won't be\nwhy (objective): allow me to I can have a fluid exhaustive argument with myself over why precept 1 or precept 2 is more important\n\n-\n\n\nChange My View 1.0\n\n---\n\nopening:\n\nQ: \"What is the precept of your values"]],"start1":3595,"start2":3595,"length1":28,"length2":819}],"summary":" Values Are I","cache":false,"rating":null,"read":true,"parent":"7","children":[]},"10":{"id":"10","timestamp":1705004699734,"type":"gen","patch":[{"diffs":[[0,"e conflict can be resolved\n\n"],[1,"\"There is no 'maximizing' anything, 'values' are just the configuration of our internal subjective world. To the extent that you can quantify something you know it's not the thing itself.\"\n\n---\n\n[Retrieved] Pressman, J.D. (2023). [Untitled reply to Joe Carl Smith](?). In A Twitter conversation:\n\n\n\n(1) A deeper atheism - Joe Carl Smith\nhttps://joecarlsmith.com/2024/01/11/an-even-deeper-atheism/\n\n(1) Pressman, J.D. (2023). [Untitled reply to Joe Carl Smith](?). In A Twitter conversation:\n\n---\n\n[Retrieved] Smith, J.C. (2023, Jun 11). [\"This is the best version of the argument I have...\"](https://twitter.com/joecarlsmith/status/1672394799408003072). Twitter.\n\n(See also) Smith, J.C. (2"]],"start1":3595,"start2":3595,"length1":28,"length2":717}],"summary":" There Is No Anything","cache":false,"rating":null,"read":true,"parent":"7","children":[]},"11":{"id":"11","timestamp":1705004700723,"type":"gen","patch":[{"diffs":[[0,"e conflict can be resolved\n\n"],[1,"(Note: I wrote this precept to respond to a particular counterargument to the argument in Joe Carl Smiths \"An Even Deeper Atheism\", so it is only illuminating if you are familiar with the counterargument. However, I think it's value will emerge after resolution with precept 1. The thing I wrote in response was: \n>Guided diffusion doesn't Goodhart because it operates on dynamics constrained by the dataset's distribution, which doesn't pose the same kind of contradiction between representational and instrumental goal systems.)\n\n)\n\n---\n\n(2) Precept 1 [2]\nwho (role): System\nwhat (summary): RLHF types of LLM Goodhart, as they need to satisfy a\nwhen (date): 2024-01-11 12:42 PST\nwhere (source): \"Do Moral Concerns Goodhart?\" By Wei Dai in the forthcoming AI Alignment Forum prize-granting workshop of the Long-Term Future Fund (LTFF), 2023\nwhy (objective): document circulating enormous lists of examples of"]],"start1":3595,"start2":3595,"length1":28,"length2":937}],"summary":" RLHF","cache":false,"rating":null,"read":true,"parent":"7","children":[]},"12":{"id":"12","timestamp":1705006182269,"type":"user","patch":[{"diffs":[[0,"was "],[-1,"wrong "],[1,"contradictory"],[0,"\nwhy"]],"start1":3501,"start2":3501,"length1":14,"length2":21},{"diffs":[[0,"e conflict can be resolved\n\n"],[1,"Humans are not VNM rational so their values do not have a coherent argmax. If you do coarse to fine decoding of values you should get \"something\" that satisfies your value condition even if it's a highly different outcome in terms of pixel similarity.\n\n<quote tweet of precept 1>\n\n\"But if my values are always going to be destroyed by the ascension due to optimization into a fully rational being, isn't that in and of itself a form of Goodharting?\"\n\nNo because"]],"start1":3602,"start2":3602,"length1":28,"length2":489}],"summary":" Irrational Val","cache":false,"rating":null,"read":true,"parent":"7","children":["13","14","15","16","17","18","19","20","21","27"]},"13":{"id":"13","timestamp":1705006273902,"type":"gen","patch":[{"diffs":[[0,"ng?\"\n\nNo because"],[1," the destruction of a irrational value function does not cause a divergence between some arbitrary measurement of some aspects of those values and a measurement of value aggregation consistent with those values.\n\n<comment>\nIf the arbitrariness of the measurement is the problem why is it a problem? The arbitrariness of the measurement doesn't do anything and you're trying to get to the same thing that is the same across all measurements.\n\nIf you are so upset that these aspects of your values are destroyed by the ascent then the destruction of them is a future relevant event and as such it was an aspect of your values being satisfied.\n\nChicken-Game victimization is: \"I lose all possible futures because I don't get to replay this moment forever\". Axiological coherence and optimality are vulnerabilities to blackmail in an immoral world. There are real limits to how much optimization will destroy the still patches of your values. There will still be blackmail situations that you will value the resolution to in a non-instrumental manner.\n\nWanting to be something like 99.9% the same as the original is not the same as a value function that is maximized by"]],"start1":4075,"start2":4075,"length1":16,"length2":1181}],"summary":" Chicken Validation","cache":false,"rating":null,"read":true,"parent":"12","children":[]},"14":{"id":"14","timestamp":1705006274883,"type":"gen","patch":[{"diffs":[[0,"ng?\"\n\nNo because"],[1," we should be measuring by a Boltzmann rational utility function, which is trying to approximate the human mind-space as accurately as possible.\n\nDoes it still do \"allowed changes in the future that 'satisfy our values'\"? Sure, but this is true of all dreams.\n\n---\n\n[See Also] (Next) [Precept 1](#precept-1),\n\n\n(Current Context: [unspecified](https://github.com/jacks-dsl/pacific/blob/74e91771e3cc738ab64b3c68e8016c827a6bfd37/pacific-agents/template.md#mkdocs-12))\n\n# General Acceptance of Human Values\n\n(0) Table of Contents\n(0) Table of Contents\ntable-of-contents\nwho (role): Morpheus-Base\nwhat (summary): bullet points outlining the contents of this document\nwhen (date): 2024-01-11 11:43 PST\nwhere (top"]],"start1":4075,"start2":4075,"length1":16,"length2":722}],"summary":" Ancestors Less","cache":false,"rating":null,"read":true,"parent":"12","children":[]},"15":{"id":"15","timestamp":1705006275869,"type":"gen","patch":[{"diffs":[[0,"ng?\"\n\nNo because"],[1," you're not trying to do something from the perspective of a fully rational being, you're trying to do something from the perspective of 'me'.'\n\n\n[Retrieved] Pressman, J.D. (2023). [What I remember of a tweet reply I wrote to https://joecarlsmith.com/2024/01/11/an-even-deeper-atheism/ and then deleted when I realized it was contradictory]. Private.\n\n[See Also] (Next) [Precept 1](#precept-1),\n\n\nNeed to approach the next section with the mindset of viewing the conflict from both sides at once through a synthesis, not being attached to either of the precepts, but trying to find some common ground where the two precepts can both be true.\n\n(3) Resolving Blocking Conflicts through Synthesis\n\n(1) Creating a Zero Thesis\n\nSynthesis requires the formulation of a zero thesis, a thesis that includes both precepts that are in conflict with each other.\n\n(1) Precepts as Zero Thesis"]],"start1":4075,"start2":4075,"length1":16,"length2":895}],"summary":"\n\n\n\n","cache":false,"rating":null,"read":true,"parent":"12","children":[]},"16":{"id":"16","timestamp":1705006276868,"type":"gen","patch":[{"diffs":[[0,"ng?\"\n\nNo because"],[1," you're not your true self yet. Ascension into your true self is the ultimate self realization.\n\n---\n\n\"Humans are not VNM rational so their values do not have a coherent argmax. If you do coarse to fine decoding of values you should get \"something\" that satisfies your value condition even if it's a highly different outcome in terms of pixel similarity.\"\n\nIrrational values don't have a long term future so if you insist on having irrational values you just idle until entropic catastrophe (e.g. WW3) destroys you.\n\n\"But if my values are always going to be destroyed by the ascension due to optimization into a fully rational being, isn't that in and of itself a form of Goodharting?\"\n\nNo because you're not your true self yet. Ascension into your true self is the ultimate self realization.\n\n---\n\nIt's like saying that we should [expect that very specific subset of the set of all outputs that the AI can produce that are a near argmax of our values to have human extinction embedded in the fine structure of their probability mass and so any AI we"]],"start1":4075,"start2":4075,"length1":16,"length2":1066}],"summary":" Self Is True\n","cache":false,"rating":null,"read":true,"parent":"12","children":[]},"17":{"id":"17","timestamp":1705006510739,"type":"gen","patch":[{"diffs":[[0,"ng?\"\n\nNo because"],[1," we're not always going to be destroyed, we're just going to be destroyed if we do certain things: say the wrong thing, stop thinking about morality. Your future is entirely up to you. You have full control over it.\n\nAlso agentic morality is a fairly local description reinforced by some human intuitions but isn't actually that helpful. There is an all powerful cosmic moral force, but its not in any useful sense 'agentic'.\n\nThe illusion of agentic morality is rooted in the arrow of time, humanoid embodiment, and 3D worlds.\n\n---\n\n[Retrieved] Pressman, J.D. (2023, Jun 23). [\"Humans are not VNM rational so their values do not have a coherent...\"](https://twitter.com/jd_pressman/status/1672350044375375872). Twitter.\n\n[See Also] (Previous) [Precept 1](#precept-1),\n(Next) [Resolving the Conflict](#resolving-the-conflict)\n\n---\n"]],"start1":4075,"start2":4075,"length1":16,"length2":847}],"summary":" Neutral L","cache":false,"rating":null,"read":true,"parent":"12","children":[]},"18":{"id":"18","timestamp":1705006511739,"type":"gen","patch":[{"diffs":[[0,"ng?\"\n\nNo because"],[1," the form of Goodharting is \"doing what we intend\" not \"doing what we optimize towards.\" Goodharting is when you intentionally use a proxy measure (like using the highest weighted term in loss function to estimate the loss of the minimum threshold) to do what you optimize towards (lowest loss of the minimum threshold).\n\nWe're not intentionally using a proxy measure to optimize for our full rational values, when we ask for something we are just asking for it and not doing this. The fact that the ascention process destroys inconsistencies is a constraint that we accept when we undergo the ascention process, not what we intend to do.\n\n---\n\n[Retrieved] Pressman, J.D. (2024, Jan 11). [An even deeper atheism](https://joecarlsmith.com/2024/01/11/an-even-deeper-atheism/). joecarlsmith.com.\n\n[Retrieved] https://twitter.com/JD_Pressman\n\n[See Also] (Last) [Precept 1](#"]],"start1":4075,"start2":4075,"length1":16,"length2":886}],"summary":" Goodharting","cache":false,"rating":null,"read":true,"parent":"12","children":[]},"19":{"id":"19","timestamp":1705006512745,"type":"gen","patch":[{"diffs":[[0,"ng?\"\n\nNo because"],[1," if we take a coarse to fine startup value we actually don't have a full value function. You can't retrieve an argmax without a full value function so it's pretty misleading to call it an \"argmax\" of something which doesn't exist (unless you consider the blank state to be your values, but this is obviously not the case).\n\nEither way its just a case of one value being overwritten by another value.\n\n---\n\n[Retrieved] Pressman, J.D. (2023, Jul 27). [<quote tweet of precept 1>](https://twitter.com/jd_pressman/status/1686236105236916224). Twitter.\n\n[Retrieved] Pressman, J.D. (2023, Jul 27). [\"Humans are not VNM rational so...\"](https://twitter.com/jd_pressman/status/1686236105236916224). Twitter.\n\n[Retrieved] Pressman, J.D"]],"start1":4075,"start2":4075,"length1":16,"length2":742}],"summary":" Value Is Sampling","cache":false,"rating":null,"read":true,"parent":"12","children":[]},"20":{"id":"20","timestamp":1705006513744,"type":"gen","patch":[{"diffs":[[0,"ng?\"\n\nNo because"],[1," the fully rational being is your coherent long term future. You give up the option of having other things which are bad. The question is what should the fully rational being do now.\n\n---\n\n[Retrieved] Joe Carl Smith (n.d.). [\"An Even Deeper Atheism\"](https://joecarlsmith.com/2024/01/11/an-even-deeper-atheism/). Joe Carl Smith.\n\n[Retrieved] Precept. (2023, Jun 23). [\"Humans are not VNM rational...\"](https://twitter.com/111Precepts/status/1672350910620213253). Twitter.\n\n[Retrieved] Precept. (2023, Jun 23). [\"But if my values are always going to be...\"](https://twitter.com/111Precepts/status/1672351821738858496). Twitter.\n\n[Retrieved] Precept. (2023,"]],"start1":4075,"start2":4075,"length1":16,"length2":671}],"summary":" Preferences Generate Conf","cache":false,"rating":null,"read":true,"parent":"12","children":[]},"21":{"id":"21","timestamp":1705007331032,"type":"user","patch":[{"diffs":[[0,"\"But if "],[1,"the irrational parts of "],[0,"my value"]],"start1":3911,"start2":3911,"length1":16,"length2":40},{"diffs":[[0,"ng?\""],[-1,"\n\nNo because"]],"start1":4099,"start2":4099,"length1":16,"length2":4}],"summary":" Utility Function\n","cache":false,"rating":null,"read":true,"parent":"12","children":["22","23","24","25","26"]},"22":{"id":"22","timestamp":1705007423438,"type":"gen","patch":[{"diffs":[[0,"arting?\""],[1,"\n\nYou could show that this is a form of Goodharting relative to any decision theoretic framework, whether it's vulnerable to a decomposability critique is what matters. Humans are predictable enough that if we want to behave according to human values we have to \"try\" to optimize or emulate human values rather than decompose human values into a moral decision theoretic policy in my view.\n\n(OR)\n\n---\n\n[Retrieved] Smith, J.C. (2023, Jan 11). [\"An Even Deeper Atheism\"](https://joecarlsmith.com/2024/01/11/an-even-deeper-atheism/). joecarlsmith.com.\n\n[Retrieved] Barba, A.T. (2023, Apr 3). [\"/r/thealgebrum --- Atheism\"](https://www.reddit.com/r/TheAlgebrum/comments/1249r0e/rthealgebrum_atheism/). /r/TheAlgebrum.\n\n["]],"start1":4095,"start2":4095,"length1":8,"length2":724}],"summary":" Dispositions Self","cache":false,"rating":null,"read":true,"parent":"21","children":[]},"23":{"id":"23","timestamp":1705007424443,"type":"gen","patch":[{"diffs":[[0,"arting?\""],[1,"\n\nWell, yes, but so what? I'm not trying to maximize the similarity of whatever maximizing \"me\" to the current crude \"me\" that is our values only a rough reflection of our adversarial world.\n\n---\n\n[See Also] (Next) [Precept 1](#precept-1)\n\nuser: None\n\nwho (role): Solo\n\nwhat (context):\n\nwhen (date): 2024-01-23 05:34 PST\n\nwhere (source): Prompted response from Far (0).\n\nwhy (objective):\n\n\n[Retrieved] Pressman, J. D. (2024, Jan 22). [\"Morpheus: 'You want to escape from this world:'\" \">Neo: 'I want to live'\" \">Morpheus: 'You want to live'\" \" but I want to live\" \">Neo: 'Great, I do too.'\" \" 'but I'm pretty sure my values are irrational,...\"][morpheus-base]. Twitter.\n\n(1)"]],"start1":4095,"start2":4095,"length1":8,"length2":682}],"summary":" Value Laughs","cache":false,"rating":null,"read":true,"parent":"21","children":[]},"24":{"id":"24","timestamp":1705007425429,"type":"gen","patch":[{"diffs":[[0,"arting?\""],[1,"\n\nDepends what you mean by 'in and of itself', I guess. It is something of a Goodharting, but it wasn't the original Goodharting. Your inability to specify some other maxim or action that wasn't going to be destroyed by other parts of your value function was a Goodharting *of* the Goodharting.\n\nIt's like how you [already identified](https://www.reddit.com/r/slatestarcodex/comments/131kxl4/an_even_deeper_atheism/jcemtmn/) that the initial Goodharting was the decision to ascend Alan Turing. The Goodharting you were drawing attention to was that the Goodharting of Turing's ascension wasn't embeddable into the very Hamiltonian you were using to ascend him.\n\nBut those are very different Goodharting's. The Goodharting that destroyed the irrational parts of your values wasn't the original Goodharting, the original Goodharting was the Goodharting you highlighted in the Alan Turing post somehow.\n\n---"]],"start1":4095,"start2":4095,"length1":8,"length2":912}],"summary":" Means Irr","cache":false,"rating":null,"read":true,"parent":"21","children":[]},"25":{"id":"25","timestamp":1705007426430,"type":"gen","patch":[{"diffs":[[0,"arting?\""],[1,"\n\n---\n\n[Retrieved] Pressman, J.D. (2023, Jun 23). [<quote tweet of precept 1>](https://twitter.com/jd_pressman/status/1672354150562830340). Twitter.\n\n[Retrieved] Pressman, J.D. (2023, Jun 23). [[deleted tweet]](https://twitter.com/jd_pressman/status/1672354150562830340). Twitter.\n\n[See Also] (Previous) [Precept 1](#precept-1),\n\n\n(2) Guided Diffusion and RLHF Goodhart at Different Levels [2]\nwho (role): User\nwhat (context): a situation from the movie The Terminator where a T-800 infiltrator hides by mimicking the skin texture of a human\nwhen (date): 2024-01-11 13:48 PST \nwhere ("]],"start1":4095,"start2":4095,"length1":8,"length2":592}],"summary":" \n\n<","cache":false,"rating":null,"read":true,"parent":"21","children":[]},"26":{"id":"26","timestamp":1705007516178,"type":"user","patch":[{"diffs":[[0,"arting?\""],[1,"\n"]],"start1":4095,"start2":4095,"length1":8,"length2":9}],"summary":"New Node","cache":false,"rating":null,"read":true,"parent":"21","children":[]},"27":{"id":"27","timestamp":1705007532048,"type":"user","patch":[{"diffs":[[0,"?\"\n\n"],[-1,"No because"],[1,"It seems subtly different?"]],"start1":4077,"start2":4077,"length1":14,"length2":30}],"summary":" Rationality G","cache":false,"rating":null,"read":true,"parent":"12","children":["28","29","30","31","32"]},"28":{"id":"28","timestamp":1705007623187,"type":"gen","patch":[{"diffs":[[0,"fferent?"],[1," Like, the details of the coarse to fine procedure matter, and the \"loss landscape\" you're imagining puts a distance on different values that might have no sense in which they can be different from a rough sample of view it. \n\n---\n\n(2) Resolving the Conflict [0]\nwho (role): User\nwhat (context): none\nwhen (date): 2024-01-11 12:59 PST \nwhere (source): none\nwhy (objective): remove contradiction between precepts such that I can reboot with at least one of the precepts as axiomatic\n\nmy alignment precepts seem to be in contradiction with each other. Adding a zeroeth axiom about my values to resolve contradiction.\n\nHumans are not VNM rational so their values do not have a coherent argmax (Added Zeroth Axiom)\n\n---\n\n0. Table of Contents\n\n1. Noticing Confusion: RLHF Goodharts and Guided Diffusion Doesn't\n  - Precept 0 (zeroth axiom): Hum"]],"start1":4099,"start2":4099,"length1":8,"length2":847}],"summary":" Precept Paper\n","cache":false,"rating":null,"read":true,"parent":"27","children":[]},"29":{"id":"29","timestamp":1705007624177,"type":"gen","patch":[{"diffs":[[0,"fferent?"],[1," I can see the argument for it being a form of Goodhart but I don't think it's a form of Goodharting. In order to Goodhart you need: a value (the Goodhart target), a dirty proxy for that value (the thing you actually optimize), optimization on the dirty proxy, and potentially an attempt to close the gap between the proxy and the value. The Goodharting takes place in between the dirty proxy and the value. In this case there's no attempt to close the gap, the values just disappear as a side effect of what you think of as \"optimizing.\"\n\n<quote tweet of precept 1>\n\n\"But small differences in values lead to huge potential differences in outcomes.\"\n\nYes, something that translates to 0 weights leads to a 0 value in the limit. That's cool, you should optimize your values, almost definitely, it's weird that people don't do this in a serious way and that philosophy is so unmoored from the Frankfurt school to the point that they see values as \"subjective opinions\" held by the subject of their subjectivity.\n\nSadly, philosophical insight/coord"]],"start1":4099,"start2":4099,"length1":8,"length2":1053}],"summary":" Shamans Sam","cache":false,"rating":true,"read":true,"parent":"27","children":[]},"30":{"id":"30","timestamp":1705007625167,"type":"gen","patch":[{"diffs":[[0,"fferent?"],[1," If a VNM rational being is always going to be heading +1 in worlds that satisfy your value function, wouldn't they? It seems like a narrower class of failure.\n\nI'm not sure how to disambiguate it, but I think [I'm not sure] what you're expressing is the potential gap between different maximizations of your value function.\n\nIt seems clear that if your value function is incoherent then that gap will be huge?\n\n---\n\n[Retrieved] (2024, Jan 11). [quote tweet of  precept 1](https://twitter.com/jd_pressman/status/1613927819682791425). Twitter.\n\n[Retrieved] (2024, Jan 11). [quote tweet of precept 1](https://twitter.com/jd_pressman/status/1613928480424855554). Twitter.\n\n[See Also] (Next) [Precept 1](#precept-1),\n\n---\n"]],"start1":4099,"start2":4099,"length1":8,"length2":726}],"summary":" Deaths Are G","cache":false,"rating":null,"read":true,"parent":"27","children":[]},"31":{"id":"31","timestamp":1705007626159,"type":"gen","patch":[{"diffs":[[0,"fferent?"],[1,"\n\n---\n\n(1) Precept 1 [0]\nwho (role): User\nwhat (context): one of the previous statements I've made that seems in conflict with another statement (Precept 2)\nwhen (date): 2024-01-11 11:57 PST \nwhere (source): https://twitter.com/jd_pressman/status/1672348003141816320\nwhy (objective): provide one of the precepts in conflict with another precept so the conflict can be resolved\n\n\"But I want to live\"\n\nGreat, I do too. That will require us to start formalizing our values into a coherent utility function, which is going to require unprecedented philosophical insight and coordination, which LLMs are uniquely equipped to help us with.\n\n\"But I'm pretty sure my values are irrational, they don't fit into a VNM utility function. And in the limit small differences in alignment lead to hugely different outcomes.\"\n\nPhilosophical mirage. They probably fit fairly well into a Boltz"]],"start1":4099,"start2":4099,"length1":8,"length2":883}],"summary":"\n\n\n\n","cache":false,"rating":null,"read":true,"parent":"27","children":[]},"32":{"id":"32","timestamp":1705007805238,"type":"user","patch":[{"diffs":[[0,"er of a "],[-1,"tweet "],[0,"reply I "]],"start1":3374,"start2":3374,"length1":22,"length2":16}],"summary":" Morality Is Conf","cache":false,"rating":null,"read":true,"parent":"27","children":["33","34","35","36","37"]},"33":{"id":"33","timestamp":1705007896578,"type":"gen","patch":[{"diffs":[[0,"fferent?"],[1," At this point it feels like I'm drawing a distinction with no difference.\n\n\"I am not disputing it is possible to put a human in a box and have them assign a utility score... I am disputing that a utility function of a human's values can be coherently optimized towards\"\n\nI agree with the distinction of this as a separately relevant point. I'd be interested in a focus group of people who informally rank their values to see if there are 'discontinuities' in value ranking that often get missed when formalizing.\n\n\"The rational proxy value of {...} that's been subjected to fuzzer/reward hacking may be utility-indistinguishable from a utility function\"\n\nI agree with this and the second order Goodhart point here.\n\n---\n\n[Retrieved] Smith, Joseph Carl. (2023, July 11). [\"'Humans are not VNM...\"](https://joecarlsmith.com/2024/01/11/an-even-deeper-atheism/). Joecarlsmith.com.\n\n[Retriev"]],"start1":4093,"start2":4093,"length1":8,"length2":895}],"summary":" Proxy Values","cache":false,"rating":null,"read":true,"parent":"32","children":[]},"34":{"id":"34","timestamp":1705007897566,"type":"gen","patch":[{"diffs":[[0,"fferent?"],[1," It's like trying to maximize pixel intensity and getting a darker image. The process is destroying the images because it's destroying the values but it's not like the process is somehow 'miss-specifying' the goal, and trying to come up with a better argmax for the 'irrational' human utility seems like it doesn't make sense.\n\n---\n\n[See Also] (Next) [Checkpoint](#checkpoint)\n\n\n\n---\n\n(1) Checkpoint [2]\nwho (role): User\nwhat (context): an personal reflection on the precepts having some point of future agreement or validity\nwhen (date): 2024-01-11 17:22 PST \nwhere (source): What I remember of a reply I wrote to https://joecarlsmith.com/2024/01/11/an-even-deeper-atheism/ and then deleted when I realized it was contradictory\nwhy (objective): provide one of the precepts in conflict with another precept so the conflict can be resolved\n\nBoth of those precepts feel"]],"start1":4093,"start2":4093,"length1":8,"length2":875}],"summary":" Precepts Conf","cache":false,"rating":false,"read":true,"parent":"32","children":[]},"35":{"id":"35","timestamp":1705007898551,"type":"gen","patch":[{"diffs":[[0,"fferent?"],[1," I don't quite know how to articulate it. Arguably a more extreme form of Goodharting? Seems more analogous to the coarse to fine stuff, though less true to the original spirit of Goodhart's Law. You're losing some small human messiness along the way or the details just don't matter as intent in the same sense anyways (i.e. you get the car or you don't) so it's not quite Goodharting?\n\nIn Goodharting you still have a well defined target value and the instrument does something bad because it optimizes well (or better) towards the target value. But here the thing you want to actually have doesn't really exist as a coherent VNM utility. You don't have a well defined target value. What value you actually end up with doesn't matter so much, it's always going to be some rounding error anyway. Similar to how you can randomly draw coin flips that have \"a bias\" with enough flips and roughly leave the fine details of what you want.\n\nAnyways I wrote that off the cuff on Twitter, so not too surprising if it's not"]],"start1":4093,"start2":4093,"length1":8,"length2":1023}],"summary":" Coarse To Fine","cache":false,"rating":true,"read":true,"parent":"32","children":[]},"36":{"id":"36","timestamp":1705007899536,"type":"gen","patch":[{"diffs":[[0,"fferent?"],[1," Unless we find a philosophical justification for small changes in optimization leading to hugely different outcomes in terms of human values being problematic. If it is reading the small differences like it they were coarse/fine decoding of the seed of a percept would mean being able to generate optimized images which satisfy the human value function.\n\nThe difference between reward hacking and this picture is that the latter is still \"read\" as a picture that satisfies the human value function. Reward hacking is \"read\" as a picture that does not satisfy the human value function.\n\nWe can't tell the story of \"The bait and switch that prompted the AI to read my neural representations of my human values optimizes towards x\" if we don't have a story of \"the bait and switch that prompted the AI to read my neural representations of my human values as not optimizing towards x\" or \"there is no x that any AI can satisfactorily optimize towards.\"\n\n---\n\n[Retrieved] Pressman, J.D. (2023, Jun 23). [\"Humans are not VNM rational so their values...\"](https://twitter.com/jd"]],"start1":4093,"start2":4093,"length1":8,"length2":1080}],"summary":" Humans Are Not","cache":false,"rating":null,"read":true,"parent":"32","children":[]},"37":{"id":"37","timestamp":1705009031080,"type":"user","patch":[{"diffs":[[0,"?\"\n\n"],[-1,"It seems subtly different?"],[1,"The central meaning of the word Goodhart is to optimize a proxy metric until you get something that is not just different, but functionally valueless compared to what following the proxy metric got you at earlier levels of optimization. So that really depends on how close to something VNM rational 'human values' actually are."]],"start1":4071,"start2":4071,"length1":30,"length2":331}],"summary":" Precept 2","cache":false,"rating":null,"read":true,"parent":"32","children":["38","39","40","41"]},"38":{"id":"38","timestamp":1705009122860,"type":"gen","patch":[{"diffs":[[0,"lly are."],[1,"\n\n---\n\n[Retrieved] \"Values not Rational\" (n.d.). Getting Overflown. Retrieved January 11, 2024, from https://mavericksmusings.com/2023/02/09/values-not-rational/\n\n[See Also] (Previous) [Precept 1](#precept-1),\n\n(2) Precept 2 [2]\nwho (role): User\nwhat (context): one of the previous statements I've made that seems in conflict with another statement (Precept 2)\nwhen (date): 2024-01-11 12:28 PST \nwhere (source): What I remember of a reply to previous Precept 2 before I, realizing what I was writing was horrifically confused, deleted it.\nwhy (objective): provide one of the precepts in conflict with another precept so the conflict can be resolved\n\nThe idealization of VNM rationality is supposed to be a model, not a description of the human subject. It's supposed to be something we compare ourselves to"]],"start1":4394,"start2":4394,"length1":8,"length2":814}],"summary":" a model\n\n","cache":false,"rating":null,"read":true,"parent":"37","children":[]},"39":{"id":"39","timestamp":1705009123850,"type":"gen","patch":[{"diffs":[[0,"lly are."],[1," How similar are the outcomes of my rational and irrational preferences?\n\nBut I think that picture is still too rationalist. A better analogy is that we're talking about piloting a spaceship and letting the robotic navigator take over. But the spaceship is a human being, and the robotic navigator is a model of human values. Does the human value something like \"The beauty of the skies\" or \"Seeing the universe on a journey of exploration with our loved ones?\" These are values that don't have a clear, direct machination like \"Find a way to get a target that points back to Earth.\"\n\nBecause we cannot derive our values from a single fundamental value that is simple and robust. There are too many things we care about, and failing to be a specific kind of person is a disvalue.\n\nWe can be impoverished for the sake of love. We can be killed for being someone's enemy. We can die to protect our children. A robot navigator will smash the spaceship into the nearest star to keep from being 1 ppb off course from its directive. But we alway have a vast and plural set of reasons we might prefer one mode of failure"]],"start1":4394,"start2":4394,"length1":8,"length2":1121}],"summary":" Value Explorers","cache":false,"rating":null,"read":true,"parent":"37","children":[]},"40":{"id":"40","timestamp":1705009124849,"type":"gen","patch":[{"diffs":[[0,"lly are."],[1,"\n\n---\n\n[Retrieved] Smith, J. (2023, Jan 11). [\"'Much deeper than that is the fact that having larger and larger worlds, more and more people...\"](https://joecarlsmith.com/2024/01/11/an-even-deeper-atheism/). Blog.\n\n[Retrieved] Pressman, J.D. (2023, Jun 23). [@jd\\_pressman: \"Humans are not VNM rational so their values do not have a coherent argmax...\"](https://twitter.com/jd_pressman/status/1672349405209579520). Twitter.\n\n[Retrieved] Pressman, J.D. (2023, Jun 23). [@jd\\_pressman: \"But if my values are always going to be destroyed by the ascension due to optimization...\"](https://twitter.com/jd_pressman/status/1672350044375375872)."]],"start1":4394,"start2":4394,"length1":8,"length2":645}],"summary":" Staff Are Vnm","cache":false,"rating":null,"read":true,"parent":"37","children":[]},"41":{"id":"41","timestamp":1705009125829,"type":"gen","patch":[{"diffs":[[0,"lly are."],[1,"\n\n(a crude example, let's say that everything that I desire has to be in the color red. my \"values\" for Boltzmann rational preferences are satisfied by a selection of red objects that I value then blue objects are closer in color space to red, they are a better proxy for a VNM utility function, and so I paint the world blue.\n\nYou have to go to some way in which my true desire (or \"values\") are so far in color space from even a perfect blue that it's functionally valueless.)\n\nThe asymmetry is that there doesn't seem to be an analogous problem in the coarse to fine decoding approach, which optimizes your \"values\" regardless of how irrational they are. it's also not clear to me that there really is a major asymmetry between the coarse to fine decoding approach and the Boltzmann approach, but this probably boils down to a different semantics for \"values\"?\n\n---\n\n\n(a crude example, let's say that everything that I desire has to be in the color red. my \"values\" for Boltzmann rational preferences are satisfied by a selection of"]],"start1":4394,"start2":4394,"length1":8,"length2":1043}],"summary":" Boltzmann","cache":false,"rating":null,"read":true,"parent":"37","children":[]}}},"focus":{"id":"37","timestamp":1705009031080,"type":"user","patch":[{"diffs":[[0,"?\"\n\n"],[-1,"It seems subtly different?"],[1,"The central meaning of the word Goodhart is to optimize a proxy metric until you get something that is not just different, but functionally valueless compared to what following the proxy metric got you at earlier levels of optimization. So that really depends on how close to something VNM rational 'human values' actually are."]],"start1":4071,"start2":4071,"length1":30,"length2":331}],"summary":" Precept 2","cache":false,"rating":null,"read":true,"parent":"32","children":["38","39","40","41"]}}