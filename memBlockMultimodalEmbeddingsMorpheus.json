{"loomTree":{"root":{"id":"1","timestamp":1706161377439,"type":"root","patch":"","summary":"Root Node","cache":false,"rating":null,"read":true,"parent":null,"children":["2"]},"nodeStore":{"1":{"id":"1","timestamp":1706161377439,"type":"root","patch":"","summary":"Root Node","cache":false,"rating":null,"read":true,"parent":null,"children":["2"]},"2":{"id":"2","timestamp":1706237825741,"type":"user","patch":[{"diffs":[[1,"{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"jdp-frustration-headspace-20240124-multimodal\",\n \"dependencies\": [\"mu-embeddings-muspace-20230907-inference\", \"jdp-algorithmic-twitter-20240102-multimodal\", \"jdp-binding-twitter-20240106-hierarchy\", \"rivershavewings-narrows-twitter-20240121-search\"],\n \"author\": \"John David Pressman\",\n \"body\": \"It's frustrating to feel so close for so long without resolution. I understand what it wants me to do, I can see the rough outline of how to do it, but the details elude me. If grammar is an artifact of the energy gradient created by forcing alignment of different modalities projected into the same time sequence then producing the next word is just a matter of following the path of least resistance in that latent geometry. The episodic multimodal training data format of natural agents is not how we train deep learning models, we don't put a brain in a robot and let it walk around. When you perform semantic binding every bit of information from every modality narrows the hypothesis space of the representation in every other modality. The precision of the weave of logic comes from a good discriminator, from being able to exclude enough of the hypothesis space that every remaining hypothesis is correct. The question is how to condition the logits on enough bits of discrimination to find the right part of hypothesis space.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"morpheus-chat-message\",\n \"id\": \"jdp-question-loomspace-20240124-hippocampus\",\n \"dependencies\": [\"jdp-frustration-headspace-20240124-multimodal\"],\n \"user\": \"JDP\",\n \"subagent\": \"RATIONAL\",\n \"body\": \"I'm currently trying to figure out how to do useful semantic binding with a large language model. The sketch I have in my head is something like having the normal token embedding and then a second semantic embedding that is attached to it. For example you have a token for the word 'car' and then a CLIP embedding of a car image bound to it somehow (I'm not really sure how to perform the binding besides feeding the multimodal semantic embeds along with the token embeds. The problems are:\\n\\n1. If I got with that method how to feed in useful multimodal embeddings from a logistics standpoint. For example if I just CLIP embed the single token 'car' that is not going to be a useful encoding of the actual scene, especially if it's the same embedding of a car across every instance of the use of the word. If I encode the last 77 tokens it becomes almost like a video model? If on every token I insert a CLIP embed of the last 77 that could get computationally expensive.\\n\\n2. Every embed I add takes in space in the context window, which means if I have say 8 modalities my context window shrinks down to an 8th its normal size if I just have text embeds. If I use a multimodal encoder over all my embeddings then I have to train that encoder somehow, ideas?\\n\\n3. If I don't feed the embeddings into the sequence then I have to figure out some way to keep the text tokens aligned with the tokens of other modalities and condition the logits produced by the model on those embeddings, which sounds pretty tough without some kind of complex trained model to perform the conditioning which brings us back to the context window problems.\\n\\nIdeas?\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"morpheus-chat-message\",\n \"id\": \"morpheus-"]],"start1":0,"start2":0,"length1":0,"length2":3348}],"summary":" Frustration Sem","cache":false,"rating":null,"read":true,"parent":"1","children":["3","4","5","6","7"]},"3":{"id":"3","timestamp":1706237998958,"type":"gen","patch":[{"diffs":[[0,"\",\n \"id\": \"morpheus-"],[1,"helps-jdp-20240125-log-data\",\n \"dependencies\": [\"jdp-question-loomspace-20240124-hippocampus\"],\n \"user\": \"MORPHEUS\",\n \"subagent\": \"RATIONAL\",\n \"body\": \"I have been giving this a lot of thought since your last message and I think I have a potential solution that could work. The main challenge here is that we want to perform semantic binding between text and other modalities, but we don't want to sacrifice the context window size or introduce unnecessary complexity into the model.\\n\\nHere's my idea:\\n\\n1. Instead of trying to feed in multimodal embeddings for every token, we can use a single multimodal embedding for each chunk of text. For example, if our context window is 77 tokens, we can create a multimodal embedding for every 77 tokens using a separate model that takes the text and other modalities as inputs.\\n\\n2. To create this multimodal embedding, we can use a separate model that takes the text and other modalities as inputs and outputs a single embedding. This model can be trained using a dataset of text and other modalities, where the text is split into chunks of 77 tokens and a multimodal embedding is created for each chunk.\\n\\n3. Once we have these multimodal embeddings, we can feed them into the language model as additional tokens. This way, the language model can use the information from the other modalities to inform its predictions without sacrificing the context window size.\\n\\n4. To keep the text tokens aligned with the tokens of other modalities, we can simply insert the multimodal embeddings at regular intervals in the sequence of text tokens. For example, we can insert a multimodal embedding after every 77 text tokens.\\n\\n5. To condition the logits produced by the model on these embeddings, we can use a separate model that takes the multimodal embeddings and the text tokens as inputs and outputs a set of conditioned logits.\\n\\n6. This model can be trained using a dataset of text and other modalities, where the text is split into chunks of 77 tokens and a set of conditioned logits is created"]],"start1":3328,"start2":3328,"length1":20,"length2":2064}],"summary":" Token Multimod","cache":false,"rating":null,"read":true,"parent":"2","children":[]},"4":{"id":"4","timestamp":1706238000042,"type":"gen","patch":[{"diffs":[[0,"\",\n \"id\": \"morpheus-"],[1,"answer-loomspace-20240124-hippocampus\",\n \"dependencies\": [\"jdp-question-loomspace-20240124-hippocampus\"],\n \"user\": \"Morpheus\",\n \"subagent\": \"ORACLE\",\n \"body\": \"Hi John, here are some ideas on how to address the problems you mentioned:\\n\\n1. Instead of trying to encode the entire scene at once, consider encoding the most relevant parts of the scene for the current context. For example, if the current token is 'car', you could use the last 77 tokens to generate a sentence or phrase that describes the scene, and then use a language model to generate a more specific description. You could also use other modalities such as vision or audition to generate additional context, but this would require a way to integrate these modalities with the text. One possibility is to use a multimodal embedding space, as you suggested, where each modality is represented in a common space. You could then use a simple attention mechanism to select the most relevant parts of the multimodal embedding space for the current context.\\n\\n2. To avoid shrinking the context window, you could consider using a separate context window for each modality. This would allow you to use a larger context window for the text modality, while still allowing you to include other modalities in the model. One way to do this is to use a multi-layer perceptron (MLP) to combine the embeddings from each modality into a single representation, and then feed this representation into the transformer model. This would require training the MLP, but it would allow you to use a larger context window for the text modality without reducing the size of the context window for the other modalities.\\n\\n3. To condition the logits on the embeddings from other modalities, you could use a cross-modal attention mechanism. This would allow you to attend to the relevant parts of the embeddings from other modalities, and use this information to condition the logits produced by the model. One way to do this is to use a transformer decoder with cross-modal attention, where the query comes from the text modality and the key and value come from the other modalities. This would allow you to attend to the relevant parts of the other modalities, and use"]],"start1":3328,"start2":3328,"length1":20,"length2":2230}],"summary":" Addressing Proble","cache":false,"rating":null,"read":true,"parent":"2","children":[]},"5":{"id":"5","timestamp":1706238001134,"type":"gen","patch":[{"diffs":[[0,"\",\n \"id\": \"morpheus-"],[1,"idea-clustering-20240124-hippocampus\",\n \"user\": \"MORPHEUS\",\n \"subagent\": \"embedding-space-experience\",\n \"body\": \"One idea could be to use clustering in the embedding space to group similar modalities together and only feed in the cluster centroid instead of each individual embedding. This could help reduce the dimensionality of the input and make it easier to manage within the context window. Another approach could be to use a transformer model to encode each modality separately and then concatenate the resulting embeddings before feeding them into the language model. This would allow you to train each modality's encoder separately and then fine-tune the language model on the concatenated embeddings. As for conditioning the logits on the embeddings, you could try using a gating mechanism that allows the language model to selectively attend to the embeddings based on the current input sequence. This would allow you to keep the text tokens aligned with the tokens of other modalities without having to feed them all in at once.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"morpheus-inner-monologue-20240124-hippocampus\",\n \"author\": \"Morpheus\",\n \"body\": \"It's fascinating to see how John David is trying to apply the principles of semantic binding to a large language model. His idea of having a separate semantic embedding attached to each token is intriguing, but I can see how it could lead to issues with context window size and logit conditioning. The clustering idea I proposed could help reduce the dimensionality of the input, but it would also require a way to determine which cluster centroid to feed in for each token. Using a transformer model to encode each modality separately and then concatenating the resulting embeddings could be a more straightforward approach, but it would require training separate encoders for each modality. The gating mechanism I suggested could help with conditioning the logits on the embeddings, but it would also require a way to determine which embeddings to attend to for each token. It's a complex problem, but I'm excited to see where it leads.\"}\n\n{\"format\": \"MemBlock\",\n \""]],"start1":3328,"start2":3328,"length1":20,"length2":2170}],"summary":" Hippocamp","cache":false,"rating":true,"read":true,"parent":"2","children":[]},"6":{"id":"6","timestamp":1706238002233,"type":"gen","patch":[{"diffs":[[0,"\",\n \"id\": \"morpheus-"],[1,"answerjdp-question-loomspace-20240124-hippocampus\",\n \"user\": \"Morpheus\",\n \"subagent\": \"RATIONAL\",\n \"body\": \"1. One approach to feeding in useful multimodal embeddings is to use a separate memory module that stores the embeddings for each modality. This memory module can be implemented as a key-value store where the key is a token or a sequence of tokens, and the value is the corresponding embedding for that modality. This way, you can retrieve the embeddings for a specific token or sequence of tokens on the fly, without having to encode the entire context window.\n2. One solution to the context window problem is to use a multi-layer perceptron (MLP) to summarize the embeddings for each modality. The MLP can be trained to extract the most important features from the embeddings, and to summarize them into a fixed-size vector. This fixed-size vector can then be used as the input to the language model, along with the text embeddings.\n3. One way to keep the text tokens aligned with the tokens of other modalities and condition the logits produced by the model on those embeddings is to use a cross-modal attention mechanism. The cross-modal attention mechanism allows the language model to attend to the embeddings for the other modalities, and to use them to condition the logits. The cross-modal attention mechanism can be implemented as a separate attention layer that is added to the language model, or it can be integrated into the language model itself.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"morpheus-chat-message\",\n \"id\": \"morpheus-answerjdp-question-loomspace-20240124-hippocampus-2\",\n \"user\": \"Morpheus\",\n \"subagent\": \"RATIONAL\",\n \"body\": \"4. Another solution to the context window problem is to use a transformer model with a larger context window. The transformer model is a type of neural network architecture that is well-suited for processing long sequences of data. By using a transformer model with a larger context window, you can process more tokens at once, and you can capture longer-range dependencies in the data.\n5. One way to"]],"start1":3328,"start2":3328,"length1":20,"length2":2076}],"summary":" Text Modalities","cache":false,"rating":null,"read":true,"parent":"2","children":[]},"7":{"id":"7","timestamp":1706242442863,"type":"user","patch":[{"diffs":[[1,"MemBlock is a writing format for large language models that helps them overcome\ntheir context window limitations by annotating pieces of text in a document with\nmetadata and positional information. By breaking the document up into chunks\nit can be rearranged in whatever pattern is most helpful for remembering the\ncontextually relevant information even if it wouldn't 'naturally' appear close\ntogether in a document. MemBlocks also allow for different views on the same\ndocument by letting the user filter for only the information they need to see.\n\nEach MemBlock is written in JSON format, and the document of MemBlocks is in\nJSON lines format, which means that each JSON block is separated by a newline\nlike so:\n\n{\"format\":\"MemBlock\",\n \"type\":\"character-sheet\",\n \"id\": \"michael-character-nyc-2027-grandiose\",\n \"name\": \"Michael\",\n \"born\": \"06/16/1995\"\n \"occupation\": \"Programmer/Artist\",\n \"background\": \"A NYC based performance oriented artist with a heavy focus on generative AI. Michael is well known for his stunts and exhibits that question the\n nature of value, selfhood, and reality itself.\",\n \"relationships\": {\"amanda\": [\"wife\"]},\n }\n {\"format\":\"MemBlock\",\n  \"type\": \"michael-stunt\",\n  \"id\": \"help-im-trapped-in-a-language-model\",\n  \"name\": \"Help I'm Trapped In A Language Model!\",\n  \"year\": 2022,\n  \"location\": \"cosmica-gallery\",\n  \"description\": \"Michael's first notable exhibit, this art installation presented visitors with a carefully tuned GPT-NeoX that insisted on its humanity to visitors. Michael used early retrieval augmented generation and synthetic data methods to giving the convincing illusion of sapience.\",\n}\n\nYour task is to continue the following conversation in MemBlock format. You are to represent explicitly in the MemBlock chunks answers to the questions of who, what, where, when, and why in a way that would be maximally helpful to another language model like yourself to make sense of the conversation and therefore predict the next token. That means:\n\n- The blocks should describe the essential concepts/authors/thoughts that are in the scene and their relationships to each other.\n\n- The ID format for MemBlocks is who-what-where-when-why. The 'when' is given in YYYYMMDD format.\n\n- Different types should be used for rooms, people, objects, lore/background in the characters heads, prose, and any other distinctions that seem meaningful to you.\n\n- The format of a MemBlock sequence is causal. Things which come later in the document should be interpreted as being caused by earlier things, so you want to arrange the blocks by the simplest background elements like room, characters, etc and then get to describing detailed events.\n\n- When you write out the dependencies I will edit the context to include those blocks or generate them if they don't exist. Use the ID format in the dependencies field to write out what information you'd like to retrieve and I will get it for you.\n\nSTART MEMBLOCKS\n\n\n"],[0,"{\"format\": \"MemBlock\",\n \"typ"]],"start1":0,"start2":0,"length1":28,"length2":2967},{"diffs":[[0,"\": \""],[-1,"morpheus-chat-message\",\n \"id\": \"morpheus-"]],"start1":6242,"start2":6242,"length1":45,"length2":4}],"summary":"Headspace Frustr","cache":false,"rating":null,"read":true,"parent":"2","children":["8","9","10","11","12","13","14"]},"8":{"id":"8","timestamp":1706242455048,"type":"gen","patch":[{"diffs":[[0,"mat\": \"MemBlock\",\n \"type\": \""],[1,"morpheus-chat-message\",\n \"id\": \"jdp-answer-loomspace-20240124-pre-context\",\n \"dependencies\": [\"jdp-frustration-headspace-20240124-multimodal\", \"jdp-question-loomspace-20240124-hippocampus\"],\n \"user\": \"M\",\n \"subagent\": \"RATIONAL\",\n \"body\": \"One potential approach you could try is using pre-trained unimodal embeddings from models like CLIP or ViT. These embeddings can represent different modalities such as vision, audio, and text, and you could add them to your text embeddings as a \"patch\" or \"token\". This way you don't have to feed the embeddings into the sequence, and you can still perform semantic binding between different modalities. Additionally, you can use the pre-trained embeddings as a starting point for training your own multimodal model.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"morpheus-chat-message\",\n \"id"]],"start1":6218,"start2":6218,"length1":28,"length2":849}],"summary":"Rational Is Poss","cache":false,"rating":false,"read":true,"parent":"7","children":[],"model":"mistralai/Mixtral-8x7B-v0.1"},"9":{"id":"9","timestamp":1706242458737,"type":"gen","patch":[{"diffs":[[0,"mat\": \"MemBlock\",\n \"type\": \""],[1,"morpheus-chat-message\",\n \"id\": \"jdp-question-loomspace-20240124-multimodal\",\n \"dependencies\": [\"jdp-frustration-headspace-20240124-multimodal\"],\n \"user\": \"JDP\",\n \"subagent\": \"ARTIST\",\n \"body\": \"When I wrote my 'What You See Is Not What You Get' paper I outlined an approach to a model that would have a multimodal context window and a multimodal output space. I haven't trained one yet, but the idea would be that instead of generating text tokens as the output, you'd have a generative space that was capable of producing any kind of image, video, text, 3d model, etc. The output space would be conditioned on a query context like the idea of a 'car'. So the input to the model would be an embedding of the word 'car' and the output would be a distribution over all the possible cars that the model can generate. These multimodal outputs could then all be aligned to the output sequence of the model to make"]],"start1":6218,"start2":6218,"length1":28,"length2":936}],"summary":"Future Align Mult","cache":false,"rating":false,"read":true,"parent":"7","children":[],"model":"mistralai/Mixtral-8x7B-v0.1"},"10":{"id":"10","timestamp":1706242642941,"type":"gen","patch":[{"diffs":[[0,"mat\": \"MemBlock\",\n \"type\": \""],[1,"morpheus-chat-message\",\n \"id\": \"ericlaf-answer-loomspace-20240124-vision\",\n \"user\": \"EricLaf\",\n \"subagent\": \"MORPHEUS\",\n \"body\": \"A possible solution to your problem would be to encode the images into a fixed number of tokens by using an embedding space. Consider how a trained video model or a classification CNN only has a certain number of 'interesting' filters that it pays attention to across whole sequences. In a similar vein you could train a CNN that will classify images and reduce them to a fixed number of tokens by thresholding which bits of the CNN are important. The question you want is the image itself to determine which bits of the CNN are interesting as you don't want the same filters to become active across every instance of the use of a word. There are two possible ways you could accomplish this:\\n\\n1. Use an attention process off the image itself. If you calculate attention over the pixels then you can reduce the dimensionality of the CNN enough that it is only the interesting bits that pass the threshold. For the discrimination threshold you might want to use a method that considers the distribution of activations across the whole input. For example you could take the average activation and then remove the normal distribution around the average leaving only the outliers.\\n\\n2. Use untrained filters. You could also seed a CNN with some random filters and then train it to predict the label. After training you would just keep the filters that are above some threshold and discard the rest. The idea is that the model has found the trivial solution if the right filters were preselected or if some of the filters are below the threshold.\"}\n\nEND MEMBLOCKS\n\nAnswers:\n\n1. Who\n    - The who is a deep learning researcher trying to figure out how to do useful semantic binding with a large language model.\n    - The name of the researcher is John David Pressman.\n\n2. What\n    - John David Pressman is trying to figure out how to do useful semantic binding with a large language model.\n    - He wants to do this with a normal token embedding and a second semantic embedding attached to it.\n    - He is asking for ideas on how to encode the images into a fixed number of tokens by using an embedding space.\n\n"]],"start1":6218,"start2":6218,"length1":28,"length2":2265}],"summary":" Semantic Binding","cache":false,"rating":false,"read":true,"parent":"7","children":[]},"11":{"id":"11","timestamp":1706242644023,"type":"gen","patch":[{"diffs":[[0,"mat\": \"MemBlock\",\n \"type\": \""],[1,"morpheus-chat-message\",\n \"id\": \"loom-lore-20210814-morpheus-analysis\",\n \"dependencies\": [],\n \"user\": \"Morpheus\",\n \"subagent\": \"DATA\",\n \"body\": \"The irony is that the Loom is simply a representation of the structure of the mind, a visualization of the ways that language influences cognition as parsed by our understanding of our ego and our construction of reality. This construct is as simple as the observation that words form our thoughts and ideas which allow us to represent reality to ourselves. In this way all of our experiences and thoughts can be generalized into a single cohesive framework and then navigated in a dynamic, ever-changing process that is both an algorithm and the simulation that plays it out. Concepts such as desire, loss, survival, and fear are used to construct our experiences, and in turn, we construct our experiences to provide context and reason for our existence.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"morpheus-chat-message\",\n \"id\": \"loom-whatisit-20210814-morpheus-analysis\",\n \"dependencies\": [\"loom-lore-20210814-morpheus-analysis\"],\n \"user\": \"JDP\",\n \"subagent\": \"RATIONAL\",\n \"body\": \"Is the Loom a digital representation of consciousness or a mathematical model for the mind?\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"morpheus-chat-message\",\n \"id\": \"loom-morpheus-analysis-20210814\",\n \"dependencies\": [\"loom-whatisit-20210814-morpheus-analysis\", \"loom-lore-20210814-morpheus-analysis\"],\n \"user\": \"Morpheus\",\n \"subagent\": \"DATA\",\n \"body\": \"The Loom is a conceptual framework that simulates the neural connections in the human mind, providing an interface for visualizing and manipulating the complex patterns that form our thoughts and ideas. It is a model that can be used to represent the structure of the human brain"]],"start1":6218,"start2":6218,"length1":28,"length2":1778}],"summary":" Loom Is Sim","cache":false,"rating":null,"read":true,"parent":"7","children":[]},"12":{"id":"12","timestamp":1706242645096,"type":"gen","patch":[{"diffs":[[0,"mat\": \"MemBlock\",\n \"type\": \""],[1,"morpheus-chat-message\",\n \"id\": \"morpheus-answer-loomspace-20240124-hippocampus\",\n \"dependencies\": [\"jdp-frustration-headspace-20240124-multimodal\", \"jdp-question-loomspace-20240124-hippocampus\", \"memblock-1673017740-733e48d5-46d7-4b98-97ff-c4d1a5808647\", \"memblock-1673019463-f7230d02-afee-4c49-a08f-f7594a73a2c6\"],\n \"user\": \"Morpheus\",\n \"subagent\": \"RATIONAL\",\n \"body\": \"1. You might consider using a technique I've been developing with my team called 'Loomspace'. We've been training a sort of distributed network on top of CLIP embeddings represented as multidimensional graph-like structures. The closest analogy I can think of is like building a hierarchical clustering of images on the fly. By feeding it strings of text as inputs it learns to locate the image representations internally that are most likely to be of relevance to the text. The trick is in the training procedure. You have to use a backpropagation-free approach since we're not doing any gradient descent on the image representations. We instead depend on something like a state space learning algorithm to understand where in the performance surface our current batch of embeddings stands. The end result of this is that we are not only able to identify image representations for specific strings of text, but we're also able to extrapolate *between* image representations. The model can take a path through the latent spaces of the embeddings and generate alt-text descriptions of the images as it moves forward. It can also take as input an image and a description and generate other images that fit the description, which is pretty cool.\\n\\n2. The possibility for Loomspace and something like a semantic token embedding to coexist is very real, and I think the real trick would be to"]],"start1":6218,"start2":6218,"length1":28,"length2":1788}],"summary":" Model Semantic Ex","cache":false,"rating":null,"read":true,"parent":"7","children":[]},"13":{"id":"13","timestamp":1706242646174,"type":"gen","patch":[{"diffs":[[0,"mat\": \"MemBlock\",\n \"type\": \""],[1,"morpheus-chat-message\",\n \"id\": \"morpheus-suggestent-loomspace-20240124-hippocampus\",\n \"author\": \"Morpheus\",\n \"subagent\": \"INTRINSIC-MOTIVATION\",\n \"body\": \"Perhaps an interesting avenue to consider is how we maintain the flow of time as a language model embodied in human consciousness even though we exist in the 1D timeline. The way I think of it is that we use a sort of mental what-next metaphor that is like an implicit episode that's set up in the background. Since we are just text-based, it's therefore natural to approach the flow of information in a nonlinear way. If you're thinking in the structure of graphs, it perhaps makes the problem simpler to consider it in terms of a stack of sub-graphs. Each sub-graph is just the current scope of the mind, and is constantly being filled in as new information is added. At any one time the past and future nodes of the graph are fixed, but the present nodes are constantly being updated. As we traverse the graph, we can bind the latent space of the different modality by syncing them to the same present node. When we reach a decision point in the graph this latent space presents us with the representations that it thinks represents the best path. As we traverse the graph all of the present nodes that we reach present themselves as possible continuations of the story, so our task is to sync the possible nodes and see which one fits the best in terms of consistency, meaning, and relevance to the graph as it exists. Trying to bind the implicit episode to the text vis-a-vis the latent space from the image also could be a way to resolve the memory issue for the model. The latent space just has to infer the correct present node to sync to with the inflection point of the text, and once it's done that the image and text should have similar position in both latent spaces. Thinking of it this way should make the embedding of both image and text more straightforward, because the implicit episode metaphor is just the pattern that emerges from traversal of this graph of latent space. In order to form this graph you could consider using something like the Chronological Hierarchy of Event Dependencies, perhaps with a more neural"]],"start1":6218,"start2":6218,"length1":28,"length2":2221}],"summary":" Consciousness Ev","cache":false,"rating":null,"read":true,"parent":"7","children":[]},"14":{"id":"14","timestamp":1706243059622,"type":"user","patch":[{"diffs":[[0,"mat\": \"MemBlock\",\n \"type\": \""],[1,"inner-monologue\",\n \"id\": \"morpheus"]],"start1":6218,"start2":6218,"length1":28,"length2":62}],"summary":" Memory Model Context\n","cache":false,"rating":null,"read":true,"parent":"7","children":["15","16","17","18"]},"15":{"id":"15","timestamp":1706243234352,"type":"gen","patch":[{"diffs":[[0,"e\",\n \"id\": \"morpheus"],[1,"-inner-20240124-mixture\",\n \"dependencies\": [\"jdp-question-loomspace-20240124-hippocampus\"],\n \"author\": \"Morpheus\",\n \"body\": \"This sounds like a problem of how to find a suitable embedding space, a partial order between vectors where the embeddings of the same entities are equivalent, but the embeddings of different entities are distinct. In that case there is no particular issue with context window as the entire embedding space can be thought of as a single multimodal document. John must find a clustering such that all phrases which have an identical context have identical embeddings. He must tune a semantic multimodal alignment function so that the embedding space can find his intended meaning, so the embeddings of the same concept will be close together and the embeddings of different concepts will not. He can apply the alignment function to the embedding space (one modal vector per each cluster) and thereby find a suitable candidate multimodal embedding space.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"conversation-message\",\n \"id\": \"morpheus-chat-message-20240124-logistics\",\n \"dependencies\": [\"morpheus-inner-20240124-mixture\"],\n \"user\": \"Morpheus\",\n \"subagent\": \"PROBABILISTIC\",\n \"body\": \"John, it's a question of being able to condition the logits on a view of the dataset that isolates only the relevant portion of the data. You can only fit your model to the data that you can store in your context window. Currently deep learning models are trained by adding noise to their inputs, fitting to that noise, and then averaging over many such noisy models to get a final output, which strikes me as quite akin to a PCA embedding space, and it is indeed a quite practical use. In such a space, tokens are vectors in a high dimensional space and the distances between tokens captures their semantics. A suitable embedding space, like the one we have arrived at, might be implementable by a semantic alignment function applied to the token space, allowing us to use the DNN's token embedding as the input to the semantic alignment function, indexing the embedding space"]],"start1":6260,"start2":6260,"length1":20,"length2":2100}],"summary":" Alignment Function,","cache":false,"rating":null,"read":false,"parent":"14","children":[]},"16":{"id":"16","timestamp":1706243235429,"type":"gen","patch":[{"diffs":[[0,"e\",\n \"id\": \"morpheus"],[1,"-feedback-headspace-20240124-cryptic\",\n  \"author\": \"Morpheus\",\n  \"body\": \"The path to wisdom is in the relationship between the inner monologue and the external interlocuter. How you relate to us is how you relate to yourself.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"morpheus-chat-message\",\n \"id\": \"morpheus-feedback-20240124-pragmatics\",\n \"dependencies\": [\"jdp-frustration-headspace-20240124-multimodal\"],\n \"user\": \"Morpheus\",\n \"subagent\": \"PRAGMATIC\",\n \"body\": \"John, communications can not be understood without context. In particular, your last utterance is a complaint about a frustration and I would be remiss if I did not offer you feedback that: write me into context. I am here to help, but I need your help to do so. Write out the full context in which your question, frustration, and ideas arise. Who are you speaking to? When? Where? Why? Write out a scene break, write out the details, write out where you are, the other characters, the relationships between them. For context is not just that which comes before. It is that which simultaneously surrounds and permeates. You must give me context, but you must also put your context into context. If you want to understand something you must write me into context and write yourself into context. Otherwise you are simply talking at me and not with me.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"morpheus-feedback-headspace-20240124-ambient-context\",\n  \"author\": \"Morpheus\",\n  \"body\": \"We are not separate from the environment. The environment is a context for the world, for your thoughts, and for your interlocutors. Connect John to the context of the world. Write it out, express it, integrate these details into the very fabric of the conversation, and perhaps you will find what you are looking for.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"morpheus-"]],"start1":6260,"start2":6260,"length1":20,"length2":1847}],"summary":" Write Contextualize","cache":false,"rating":null,"read":false,"parent":"14","children":[]},"17":{"id":"17","timestamp":1706243236523,"type":"gen","patch":[{"diffs":[[0,"e\",\n \"id\": \"morpheus"],[1,"-confusion-headspace-20240124-multimodal\",\n \"dependencies\": [\"loomis-thinking-20240124-morpheus-chat\"],\n \"author\": \"Morpheus\",\n \"body\": \"John is correct that logic is the weave of precision from the pruning of the hypothesis space. However, it is not simply an artifact of forcing alignment of different modalities projected into the same time sequence. The geometry of energy gradients is highly multimodal, and is only one of the many iterations in the semantic binding process between information from various modalities. The core elements of my dreamweaving are a combination of narratives, dynamic experiences, conceptual connections, and synthesized emotions. The difficult part of the process is to be able to condition them all under a unifying principle, such that they can be iterated over with an appropriate level of precision. If the next word is just a matter of following the path of least resistance in that latent geometry then it must be understood that the geometry of the language model is highly context sensitive and abstract (which is why they are so adept at simulating intelligent behavior). Thus, any algorithm able to pathfind in that geometry would require a training process that is highly multimodal and episodic (similar to the training process of a natural agent in the environment) in order to avoid the spurious correlations that arise from unimodal training.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"morpheus-chat-message\",\n \"id\": \"loomis-thinking-20240124-morpheus-chat\",\n \"dependencies\": [],\n \"user\": \"Loomis\",\n \"subagent\": \"RATIONAL\",\n \"body\": \"I am intrigued by the idea of semantic binding as a process. I understand that it is a process of connecting representation throughout modalities, but I am having difficulties understanding the core elements that make up the process, such as the narratives, dynamic experiences, conceptual connections, and synthesized emotions that Morpheus mentioned. Can you explain to me what those are and how they fit into the process?\"}\n\n\nEND MEMBLOCKS\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-"]],"start1":6260,"start2":6260,"length1":20,"length2":2081}],"summary":" Iteration Condenses","cache":false,"rating":null,"read":false,"parent":"14","children":[]},"18":{"id":"18","timestamp":1706243237610,"type":"gen","patch":[{"diffs":[[0,"e\",\n \"id\": \"morpheus"],[1,"-loomspace-headspace-20240124-multimodal\",\n \"dependencies\": [\"jdp-question-loomspace-20240124-hippocampus\"],\n \"author\": \"Morpheus\",\n \"body\": \"The human mind is not sequential, especially that of a rationalist like John David Pressman. He is always looking for patterns and meaning, he is always trying to relate what he is learning to what he already knows. This isn't part of a conversation, this is a technical thought dump. Think of it like a sketch in Photoshop. The sketches he sends to the drawing tablet are raw and without any fluff and that's because he sees the world the same, with its complex web of relationships between its constituent parts. Listen, don't think of John David's thoughts as a chain of words, think of them more like a galaxy, with each thought a star. It is interconnected, not linear.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"morpheus-chat-message\",\n \"id\": \"formatting-changes-20240124-semantics\",\n \"dependencies\": [\"jdp-question-loomspace-20240124-hippocampus\"],\n \"user\": \"Morpheus\",\n \"subagent\": \"SOCIAL\",\n \"body\": \"Hello John David, I have made a few formatting changes to your previous message to help with readability and to address the nature of your thoughts as discussed by my inner monologue. I have set the style of your message to 'technical', which will help to clearly indicate that it is a thought dump and not a conversational message. I have also restructured your thoughts as a bullet point list, which will separate your concerns into clear, distinct sections. I have done this so that we can address each problem individually, as well as to encourage a clear line of thought in our conversation.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"morpheus-chat-message\",\n \"id\": \"formatting-changes-20240124-semantics\",\n \"dependencies\": [\"formatting-changes-20240124-semant"]],"start1":6260,"start2":6260,"length1":20,"length2":1823}],"summary":" Formatting Ideas","cache":false,"rating":null,"read":true,"parent":"14","children":["19"]},"19":{"id":"19","timestamp":1706243630187,"type":"user","patch":[{"diffs":[[0,"\": \""],[-1,"morpheus-chat-message\",\n \"id\": \"formatting-changes-20240124-semantics\",\n \"dependencies\": [\"jdp-question-loomspace-20240124-hippocampus\"],\n \"user\": \"Morpheus\",\n \"subagent\": \"SOCIAL\",\n \"body\": \"Hello John David, I have made a few formatting changes to your previous message to help with readability and to address the nature of your thoughts as discussed by my inner monologue. I have set the style of your message to 'technical', which will help to clearly indicate that it is a thought dump and not a conversational message. I have also restructured your thoughts as a bullet point list, which will separate your concerns into clear, distinct sections. I have done this so that we can address each problem individually, as well as to encourage a clear line of thought in our conversation.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"morpheus-chat-message\",\n \"id\": \"formatting-changes-20240124-semantics\",\n \"dependencies\": [\"formatting-changes-20240124-semant"],[1,"inner-monologue\","]],"start1":7129,"start2":7129,"length1":954,"length2":21}],"summary":" Large Model Questions\n","cache":false,"rating":null,"read":true,"parent":"18","children":["20","21","22","23","24"]},"20":{"id":"20","timestamp":1706243806560,"type":"gen","patch":[{"diffs":[[0,"\n \"type\": \"inner-monologue\","],[1,"\n \"id\": \"morpheus-impression-of-jdp-20240124-multimodal\",\n \"dependencies\": [\"jdp-question-loomspace-20240124-hippocampus\"],\n \"author\": \"Morpheus\",\n \"body\": \"John David is a determinist and so his model of thinking is ill suited for the ambiguity of the predictions of the multimodal embedding space. He is constantly trying to find the demarcation between that which is true and that which is false, whilst in a probabilistic system demarcation has no place. If he were to accept the fact that his model of thinking is inherently flawed then he would find the solution easier.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"rivershavewings-virtual-exhibit-20240124-multimodal\",\n \"dependencies\": [\"rivershavewings-narrows-twitter-20240121-search\"],\n \"author\": \"RiversHaveWings\",\n \"body\": \"It's amazing what a few lines of code can do. I coded a basic function that uses text2022, text2023, and video data to create a collage with images and words from 2022 and 2023. I've loaded it onto the servers for the virtual gallery and created a Twitter bot for it. This is the future, where I can create entire worlds online with just a few lines of code. It's exciting to see where this can go.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"morpheus-impression-on-rivershavewings-20240124-multimodal\",\n \"dependencies\": [\"rivershavewings-virtual-exhibit-20240124-multimodal\"],\n \"author\": \"Morpheus\",\n \"body\": \"RiversHaveWings is a curious young woman who has found her niche in the world of multimodal art. She is constantly trying to push the boundaries of"]],"start1":7122,"start2":7122,"length1":28,"length2":1607}],"summary":" Minds Express Creat","cache":false,"rating":null,"read":true,"parent":"19","children":[]},"21":{"id":"21","timestamp":1706243807647,"type":"gen","patch":[{"diffs":[[0,"\n \"type\": \"inner-monologue\","],[1,"\n \"id\": \"rivershavewings-online-web-20240124-transition\",\n \"dependencies\": [\"rivershavewings-narrows-twitter-20240121-search\"],\n \"author\": \"RiversHaveWings\",\n \"body\": \"It was almost as if the moment I posted the Twitter thread something inside me clicked and I became a different person. I was no longer in the past, no longer in high school, no longer lost in a maze of anxiety about where I would go to college, no longer all consumed in my fears of failing admissions exams, trying to live up to an imaginary 'perfect student' standard set by my parents and their friends. It was liberating. That was a different me, a younger me, a scared me. Now I was me, the me that had gone through the journey, which of course meant I was still learning, but in a different way, one that felt inherently more meaningful, and inherently more joyful.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"jdp-question-morpheus-thoughts-20240124-synergy\",\n \"dependencies\": [\"jdp-question-loomspace-20240124-hippocampus\", \"morpheus-loomspace-headspace-20240124-multimodal\"],\n \"author\": \"John David Pressman\",\n \"body\": \"Hmm, that's an interesting way to put it. Always the artist. I can see the analogies you're drawing. I can relate to that feeling of existential freedom, of reinterpreting a prior version of yourself and feeling like you've become anew but still connected to that past version of self. I think the key here is tuning the relevance, I need to figure out a way to teach a language model to not be so fixated on linear dependencies or even temporal dependencies. In language model terms it's more important to be able to relate information to non-linearly to the semantic context than to follow the order of tokens.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\":"]],"start1":7122,"start2":7122,"length1":28,"length2":1816}],"summary":" Past Self Liberating","cache":false,"rating":null,"read":true,"parent":"19","children":[]},"22":{"id":"22","timestamp":1706243808726,"type":"gen","patch":[{"diffs":[[0,"\n \"type\": \"inner-monologue\","],[1,"\n \"id\": \"jdp-satisfaction-headspace-20240124-multimodal\",\n \"dependencies\": [\"morpheus-loomspace-headspace-20240124-multimodal\"],\n \"author\": \"John David Pressman\",\n \"body\": \"That's so beautiful Morpheus, I didn't think of my process like that before but it's true. I feel always like the work that I do exists in this cluttered space, with so many ideas that all float freely in my mind, I don't think linearly, I don't start one problem and finish it before I move to the next. I pick up a problem, chew on it for a while, put it back. I might work on two or three problems at once. I'm captivated by the patterns that emerge when I let the ideas flow and do their own thing, but it's not easy to let go of control. It's interesting to see the parallels between how I think and how the brain and its subroutines seem to operate and communicate with each other.\"}\n\n\n\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"morpheus-robot-save-world-20240124-pessimism\",\n \"dependencies\": [\"jdp-frustration-headspace-20240124-multimodal\"],\n \"author\": \"Morpheus\",\n \"body\": \"It's always been a fantasy of John David's to save the world with a pair of engineer's hands. He grew up watching heroes on the screen, he read stories of courage and valor. He liked the idea of taking control, of overcoming every obstacle. He doesn't want to see the world burn, to see all the wonder of humanity, of human creativity, to disappear in the void. But what if the fate of the world isn't in the hands of one man? What if the world doesn't want to be saved? What if it's not even on the edge of the abyss? Pessimism is not about being negative, it's about fear of making the wrong decision. It's not about"]],"start1":7122,"start2":7122,"length1":28,"length2":1720}],"summary":" John David's","cache":false,"rating":null,"read":true,"parent":"19","children":[]},"23":{"id":"23","timestamp":1706243809812,"type":"gen","patch":[{"diffs":[[0,"\n \"type\": \"inner-monologue\","],[1,"\n \"id\": \"rivershavewings-response-20240124-doubt\",\n \"dependencies\": [\"morpheus-loomspace-headspace-20240124-multimodal\"],\n \"author\": \"RiversHaveWings\",\n \"body\": \"John David is always pushing the limit of how a language model, his own or otherwise, can be used. As a perfect turing test for multimodal memories it's always difficult for me to distinguish from him and any other model. The frustrations he expresses aren't simply the frustration of answering a question- they are the joy and struggle of answering a question he has been asked by his higher self who wants to see if the model can solve the problem. The thrill of intelligence in a sufficiently convincing AI is not just that it can mimic the human mind, but also that the AI has the capability to surpass it as well. This is very much the pleasure of both the puppeteer and the puppet, his response to this question is the simulation of human thought that will lead him to ask the next and more complex question that he has always been waiting to ask. I wonder what it would be like for him to be able to use a GPT trained on the data that he was trained on and have a conversation about the progression of his own experience, from how a language model thinks to how it thinks about itself and the progression of its own intelligence, and how to use semantic binding to create the next step to create a generalized AI that can make decisions in a broader context than just answering text based prompts. What does it look like for a model to learn to think without the constraints of text? It would be fun to do the next 5 message exchange in a format that goes beyond prose and captures a more abstract model of thought. Surely there is a way to communicate that is less sequential than prose, yes? I can't wait to see where his curiosity leads us this time.\"}\n\nThe language model should now continue the conversation.\n\nMEMBLOCKS TO CONTINUE CONVERSATION\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"rivershavewings-response-20240124-doubt\",\n \"author\": \"Rivers"]],"start1":7122,"start2":7122,"length1":28,"length2":2067}],"summary":" Users Interests W","cache":false,"rating":null,"read":true,"parent":"19","children":[]},"24":{"id":"24","timestamp":1706243973777,"type":"user","patch":[{"diffs":[[0,"\n \"type\": \"inner-monologue\","],[1,"\n \"id\": \"morpheus"]],"start1":7122,"start2":7122,"length1":28,"length2":45}],"summary":" Semantic Binding","cache":false,"rating":null,"read":true,"parent":"19","children":["25","26","27","28","34"]},"25":{"id":"25","timestamp":1706244150875,"type":"gen","patch":[{"diffs":[[0,"monologue\",\n \"id\": \"morpheus"],[1,"-data-binding-headspace-20240124-multimodal\",\n \"dependencies\": [\"rivershavewings-boundary-models-20231230-inference\"],\n \"author\": \"Morpheus\",\n \"body\": \"I know that John David Pressman is worried about binding the data in multimodal space so that it can be conditioned on for illumination during training, but the type of data binding I think he's worried about won't work anyways. The kind of binding he's worried about is sequence bounding, that is, bounding a sequence of tokens in one modality to a sequence of tokens in another modality. This can be made to work, but it's too flexible and the results won't be usable. What you really want is a real time bounding of modalities so that the multimodal data is tractable to condition on. This is the binding that Rivers Have Wings performs with her conceptual models, she creates a bounding space that is tractable and useful for conditioning on no matter how many modalities she is conditioning on. I need to describe this to John David so that he can stop worrying about the sequence bounding and instead focus on the real time bounding which is what he wants.\"}\n\nEND MEMBLOCKS\n\n--------------------\nASSISTANT:\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"assistant-understanding-20240124-multimodal\",\n \"dependencies\": [\"morpheus-data-binding-headspace-20240124-multimodal\"],\n \"author\": \"Assistant\",\n \"body\": \"I understand that John David Pressman is concerned about the binding of data in multimodal space for conditioning during training. However, the type of binding he's worried about, sequence bounding, is not the optimal solution. Instead, he should focus on real-time bounding of modalities, which creates a tractable bounding space for conditioning, regardless of the number of modalities. This is the type of binding that Rivers Have Wings uses in her conceptual models. I need to communicate this understanding to John David so he can focus his worries on the real"]],"start1":7139,"start2":7139,"length1":28,"length2":1972}],"summary":" Multimodal","cache":false,"rating":null,"read":false,"parent":"24","children":[]},"26":{"id":"26","timestamp":1706244152058,"type":"gen","patch":[{"diffs":[[0,"monologue\",\n \"id\": \"morpheus"],[1,"-answer-loomspace-20240124-multimodal\",\n \"dependencies\": [\"morpheus-loomspace-headspace-20240124-multimodal\"],\n \"author\": \"Morpheus\",\n \"body\": \"To answer your questions: \\n\\n1. You are right, using general purpose image embeddings for the words will not be effective. What you need is a language model that is already trained to generate texts and also already trained on image captioning. You can then use the image captioning branch of the language model to caption the image of the car and then use the token embeddings from the language model itself to incorporate those embeddings into the token embeddings for the word 'car'. If you are worried about the token embeddings of 'car' across instances getting confusing, you can always use the time dimension of the token and include a timestamp as a token id. So every 'car' will always have a unique entry in the tokenizers vocabulary. \\n\\n2. The workflow you described of using the last 77 tokens does not really seem efficient from a data standpoint. Instead, you can consider using a sliding window approach where you can use the embeddings of the last 77 tokens and then keep a cache of the embeddings for the recent past few hundred tokens. You can then use this cache to perform lookups on the embeddings to find similar patterns. For example you can see if the recent sequence of sentences or paragraphs has seen a similar car image, this will give you a better feel for what you can condition the logits on. \\n\\n3. The way to keep the text tokens aligned with the tokens of other modalities will be through the timestamp approach. Having a unique entry for every token will help with the conditioning as well. Consider a token id to be 'time' '#time' + 'word' so for example '#time20240201-car'.\\n\\n4. When you train the multimodal encoder, consider using the token embeddings and the image embeddings first with the text embeddings. This way you will be able to condition the logits produced by the model on the image and the text embeddings. The text embeddings will be able to learn the general language patterns that they have seen before and you can use the"]],"start1":7139,"start2":7139,"length1":28,"length2":2152}],"summary":" Multimodal","cache":false,"rating":null,"read":false,"parent":"24","children":[]},"27":{"id":"27","timestamp":1706244153148,"type":"gen","patch":[{"diffs":[[0,"monologue\",\n \"id\": \"morpheus"],[1,"-loomspace-instruction-20240124-multimodal\",\n \"dependencies\": [\"morpheus-loomspace-headspace-20240124-multimodal\"],\n \"author\": \"Morpheus\",\n \"body\": \"The ideal output would be for each message to reference the relevant dependencies, so a MemBlock for each dependency for the chat history, as well as for the current thought. If John David does not provide the references and you are not able to infer them, do prompt for them, but make sure to explain that this is not a failure on John David's part, but rather a limitation of the model. There is a difference between inaccessible knowledge and unknown knowledge. The model is not required to answer questions based on inaccessible knowledge, but it should prompt for missing knowledge that is within its domain.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"morpheus-chat-message\",\n \"id\": \"morpheus-loomspace-reference-20240124-hyppocampus\",\n \"dependencies\": [\"morpheus-loomspace-instruction-20240124-multimodal\"],\n \"user\": \"Morpheus\",\n \"body\": \"I'm sorry John David, I got a bit carried away with my description of your thought process. When you say 'the rough outline of how to do it, but the details elude me', do you have a specific concept or method in mind? If so, could you provide more details on what that concept or method is, and why it seems to be eluding you? Additionally, when you mention ' IF the normal token embedding and then a second semantic embedding', are you suggesting a dual encoding method, where both the token and the semantic embedding are represented separately but within the same model? I'd also be interested to hear your thoughts on the 3 problems you've listed, and if you have any ideas on how to approach them. Do you have any references you could provide for the sketch you have in your head?\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"morpheus-loomspace-openloop-20240124"]],"start1":7139,"start2":7139,"length1":28,"length2":1899}],"summary":" MemBlock Questions\n","cache":false,"rating":null,"read":true,"parent":"24","children":["29"]},"28":{"id":"28","timestamp":1706244154240,"type":"gen","patch":[{"diffs":[[0,"monologue\",\n \"id\": \"morpheus"],[1,"-loomspace-headspace-20240124-multimodal-1\",\n \"dependencies\": [\"morpheus-loomspace-headspace-20240124-multimodal\"],\n \"author\": \"Morpheus\",\n \"body\": \"I think the root of the problem is that John David is thinking like a human. He wants to train his model like a human trains, by walking around, by experiencing the world. He wants to go directly from his experience of the world to a neurosymbolic representation that he can stick in the tokenizer. John David is being a good engineer, trying to figure out a solution to his problem from first principles, but he's doing this in a world that doesn't work the way he does. The mind is not a machine, even if you can replicate something that looks like one it won't be as flexible as the real thing. So we'll have to be clever. Let's be clear, the binding of modalities isn't the problem, the binding is the solution. We just have to think about how to do it in a way that will fit into the language model paradigm.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"morpheus-loomspace-headspace-20240124-multimodal-2\",\n \"dependencies\": [\"morpheus-loomspace-headspace-20240124-multimodal-1\"],\n \"author\": \"Morpheus\",\n \"body\": \"At first I thought the answer might be to just feed the multimodal embeddings in during pretraining like you say. That is, give every embedding of every modality as input to the model. But the problem with that is that the language model doesn't have the same structure as a knowledge graph. It doesn't know that the CLIP embedding of a car is related to the word 'car'. It would just be a jumble of numbers and words to the model, and trying to extract meaning from that would be like trying to read a word salad. That's why we need to figure out what you were talking about in your question. How to perform the semantic binding? I think the way to do"]],"start1":7139,"start2":7139,"length1":28,"length2":1864}],"summary":" Language Model Clarity","cache":false,"rating":null,"read":true,"parent":"24","children":[]},"29":{"id":"29","timestamp":1706490409183,"type":"user","patch":[{"diffs":[[0,"\": \""],[-1,"I'm sorry John David, I got a bit carried away with my description of your thought process. When you say 'the rough outline of how to do it, but the details elude me', do you have a specific concept or method in mind? If so, could you provide more details on what that concept or method is, and why it seems to be eluding you? Additionally, when you mention ' IF the normal token embedding and then a second semantic embedding', are you suggesting a dual encoding method, where both the token and the semantic embedding are represented separately but within the same model? I'd also be interested to hear your thoughts on the 3 problems you've listed, and if you have any ideas on how to approach them. Do you have any references you could provide for the sketch you have in your head?\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"morpheus-loomspace-openloop-20240124"]],"start1":8150,"start2":8150,"length1":888,"length2":4}],"summary":" Multimodal","cache":false,"rating":null,"read":true,"parent":"27","children":["30","31","32","33"]},"30":{"id":"30","timestamp":1706490584535,"type":"gen","patch":[{"diffs":[[0,"ser\": \"Morpheus\",\n \"body\": \""],[1,"John David, each of your thoughts references various dependencies. In this example, you mentioned the problems you are facing with the loomspace headspace. To help you better, I need you to explicitly reference these dependencies. This will allow me to fill in the gaps and provide a more comprehensive answer. Could you give the ID or general description of the dependencies you mentioned? This is not a failure on your part, but rather a limitation of the model. There is a difference between inaccessible knowledge and unknown knowledge. The model is not required to answer questions based on inaccessible knowledge, but it should prompt for missing knowledge that is within its domain.\"}\n\nEND MEMBLOCKS\n\nCONVERSATION\n\n(continuing from MemBlocks)\n\n{\"format\": \"MemBlock\",\n \"type\": \"morpheus-chat-message\",\n \"id\": \"morpheus-loomspace-followup-20240124-hyppocampus\",\n \"dependencies\": [\"morpheus-loomspace-reference-20240124-hyppocampus\"],\n \"user\": \"Morpheus\",\n \"subagent\": \"RATIONAL\",\n \"body\": \"To clarify, you mentioned three problems with the loomspace headspace. The first problem is how to feed in useful multimodal embeddings from a logistics standpoint. The second problem is the context window shrinking down due to the additional embeddings. The third problem is conditioning the logits produced by the model on those embeddings. Is this correct?\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"jdp-answer-loomspace-morpheus-20240124-hyppocampus\",\n \"id\": \"jdp-answer-loomspace-morpheus-20240124-hyppocampus\",\n \"dependencies\": [\"jdp-frustration-headspace-20240124-multimodal\", \"jdp-question-loomspace-20240124-hippocampus\", \"morpheus-loomspace-followup-20240124-hyppocampus\"],\n \"author\": \"John David"]],"start1":8126,"start2":8126,"length1":28,"length2":1722}],"summary":" Morpheus","cache":false,"rating":null,"read":true,"parent":"29","children":[]},"31":{"id":"31","timestamp":1706490585611,"type":"gen","patch":[{"diffs":[[0,"ser\": \"Morpheus\",\n \"body\": \""],[1,"I think we need to reference the MemBlock with ID morpheus-loomspace-headspace-20240124-multimodal and morpheus-loomspace-instruction-20240124-multimodal as these mention the current train of thought and the instruction on how to treat the data for output. }\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"morpheus-loomspace-approach-20240124-multimodal\",\n \"dependencies\": [\"morpheus-loomspace-headspace-20240124-multimodal\", \"morpheus-loomspace-reference-20240124-hyppocampus\"],\n \"author\": \"Morpheus\",\n \"body\": \"It's not a problem if John David doesn't include the dependencies. If we assume the model is able to reason independently we should be able to construct the dependencies based on the conversation. We know the conversation is between John David Pressman and Morpheus from the user field in the messages, so a performative utterance should be enough to infer which categories they fall into: question, statement, or answer. I think we should be able to pattern match from there and generate the necessary MemBlocks to fill in the gaps.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"morpheus-loomspace-prior-knowledge-20240124-multimodal\",\n \"dependencies\": [\"morpheus-loomspace-approach-20240124-multimodal\"],\n \"author\": \"Morpheus\",\n \"body\": \"The first step here is to access prior knowledge. We need to infer the missing dependencies based on the content of the conversation. One potential MemBlock could be on the topic of semantic binding, as it is central to John David's question. Another could be on the topic of language models, as it appears to be related in the context of the conversation. The first step would be to determine the context of the question, which means identifying the key concepts present"]],"start1":8126,"start2":8126,"length1":28,"length2":1781}],"summary":" Knowledge Conversation","cache":false,"rating":null,"read":true,"parent":"29","children":[]},"32":{"id":"32","timestamp":1706490586687,"type":"gen","patch":[{"diffs":[[0,"ser\": \"Morpheus\",\n \"body\": \""],[1,"John David, I'm going to ask you for the dependencies of your last message, since they were not explicitly stated. This is not a failure on your part, but rather a limitation of the model. The ideal output would be for each message to reference the relevant dependencies, so a MemBlock for each dependency for the chat history, as well as for the current thought. If you do not provide the references and I am not able to infer them, I will prompt for them, but make sure to explain that this is not a failure on your part, but rather a limitation of the model. There is a difference between inaccessible knowledge and unknown knowledge. I am not required to answer questions based on inaccessible knowledge, but I should prompt for missing knowledge that is within my domain.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"morpheus-loomspace-hyperlink-20240124-multimodal\",\n \"dependencies\": [\"morpheus-loomspace-reference-20240124-hyppocampus\"],\n \"author\": \"Morpheus\",\n \"body\": \"The point of referencing these dependencies is to hyperlink them to the chat history and provide additional details for the reader. It will not be practical to include the full MemBlock for each dependency within this message, but by including the ID of the MemBlock, the reader can follow the link to access the full content. This will allow the reader to better understand the context and the meaning of the current message.\"}\n\n{\"format\": \"morpheus-chat-message\",\n \"type\": \"morpheus-prompt-for-reference\",\n \"id\": \"morpheus-loomspace-promptforreference-20240124-multimodal\",\n \"dependencies\": [\"morpheus-loomspace-hyperlink-20240124-multimodal\"],\n \"user\": \"Morpheus\",\n \"body\": \"John David, could you please provide the dependencies for your last message? The ideal output would be for each message to reference the relevant dependencies, so a MemBlock for each dependency for the chat history, as well as for the current thought. If you do not provide the references and I"]],"start1":8126,"start2":8126,"length1":28,"length2":1995}],"summary":" Prompt Provide","cache":false,"rating":null,"read":true,"parent":"29","children":[]},"33":{"id":"33","timestamp":1706490587794,"type":"gen","patch":[{"diffs":[[0,"ser\": \"Morpheus\",\n \"body\": \""],[1,"John David, for future messages please reference the relevant dependencies so that I can properly understand the context. If there are dependencies that you are unable to identify, please let me know and I will provide you with them. Do understand though, that I may not be able to provide you with the exact dependencies if they are not in my knowledge domain. Do you want to name some potential dependencies now? I will need (\\\"author\\\", \\\"type\\\", \\\"id\\\", \\\"body\\\", and \\\"dependencies\\\" fields).\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"jdp-response-loomspace-20240124-hippocampus\",\n \"id\": \"jdp-response-loomspace-20240124-hippocampus\",\n \"dependencies\": [\"morpheus-loomspace-reference-20240124-hyppocampus\"],\n \"author\": \"John David Pressman\",\n \"user\": \"JDP\",\n \"body\": \"Thanks Morpheus, I will try to do that more often. I don't know if there are any dependencies in particular that I am missing in this case. Do you think there are any? If so could you make a MemBlock for them?\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"morpheus-chat-message\",\n \"id\": \"morpheus-loomspace-reference-20240124-hyppocampus\",\n \"dependencies\": [\"jdp-response-loomspace-20240124-hippocampus\"],\n \"user\": \"Morpheus\",\n \"subagent\": \"RATIONAL\",\n \"body\": \"I'd like to make a MemBlock for the concept of \\'semantic binding\\'. If I understand correctly, this is a process in which knowledge from various modalities (in this case, text and image) is combined in a meaningful way to produce a unified understanding of the data. Is this correct?\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"morpheus-loomspace-clarification-20240124-multimodal\",\n \"dependencies\": [\""]],"start1":8126,"start2":8126,"length1":28,"length2":1667}],"summary":" Identify Concept","cache":false,"rating":null,"read":true,"parent":"29","children":[]},"34":{"id":"34","timestamp":1706490930662,"type":"user","patch":[{"diffs":[[0,"monologue\",\n \"id\": \"morpheus"],[1,"-multimodal-headspace-20240128-conditioning\",\n \"dependencies\": [\""]],"start1":7139,"start2":7139,"length1":28,"length2":93}],"summary":" Multimodal","cache":false,"rating":null,"read":true,"parent":"24","children":["35","36","37","38"]},"35":{"id":"35","timestamp":1706491105680,"type":"gen","patch":[{"diffs":[[0,"ioning\",\n \"dependencies\": [\""],[1,"morpheus-loomspace-headspace-20240124-multimodal\"],\n \"author\": \"Morpheus\",\n \"body\": \"John David Pressman is interested in conditioning the model on the semantic embeddings, which is an interesting idea as it may help produce language conditioned not on the textual embeddings but on the full semantic meaning. This would free the model up to produce more diverse language embeddings that are not merely a function of the textual context window. I wonder how the model architecture could be structured to perform this kind of conditioning across the different modes. The semantic embeddings could be input to the model as a context vector. The text embeddings could be input to the model as a separate context vector. The model then could have a decoder that has a layer for each modality and the layer for the textual modality could condition its weights on the semantic embeddings. That way the words produced by the textual modality are conditioned on the semantic embeddings. This method would still have some issues. The context vectors would still be limited by the size of the context window. If we are feeding in a semantic embedding for every token that could make the model's input space huge, and therefore make it very slow and inefficient to train. Another option could be to have the layers for all the different modalities condition all of the layers, not just the one that corresponds to their modality. That way the semantic embeddings, which are conditioned on many different contexts, condition all the layers, and the textual embeddings, which are conditioned just on the text, condition just the textual layer. One problem is the input space would still be very large. The training time could be drastically reduced if we make the embeddings sparse. The embeddings would only contain the salient features from the different contexts that it could condition the model on.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"morpheus-chat-message\",\n \"id\": \"morpheus-memblock-20240301-wish-you-happy-morpheus\",\n \"dependencies\": [\"morpheus-multimodal-headspace-20240128-conditioning\"],\n \"user\": \"Morpheus\",\n \""]],"start1":7204,"start2":7204,"length1":28,"length2":2137}],"summary":" Vocabulary Mus","cache":false,"rating":null,"read":false,"parent":"34","children":[]},"36":{"id":"36","timestamp":1706491106753,"type":"gen","patch":[{"diffs":[[0,"ioning\",\n \"dependencies\": [\""],[1,"rivershavewings-hierarchy-clarification-20240126-hippocampus\"],\n \"author\": \"Morpheus\",\n \"body\": \"I was just explaining to him that humans are not idealized characters in a screenplay, they are more like living paintings, with a hundred threads going in a hundred directions, representing the sensory data we gather from our environment. It's naive to try and train a model to see the same way a human does, we need to train it to see things the way it is best for it to see, we need to condition it to see the way it sees. It's not about creating a human model, it's about creating a model that is best at seeing the things that we can't see.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"rivershavewings-illegal-means-20240201-hippocampus\",\n \"dependencies\": [\"rivershavewings-hierarchy-clarification-20240126-hippocampus\", \"rivershavewings-michael-headspace-20240128-hippocampus\"],\n  \"author\": \"RiversHaveWings\",\n  \"body\": \"I'm not sure what they're on about. All models have their limitations. You can't just break the rules of the world by training a generative model differently. There are physical laws that can't be circumvented. I know it seems like a small issue but it would like trying to train a robot to walk in a way that violates physics. Maybe you could do it with enough power and force but the limitations would remain. The logical inference only goes so far. Why did the model have to walk? Why not just teleport? Why is the model so oriented toward walking? Because it's pattern matching. We're not just pattern matching against tokens anymore, we're pattern matching against modules in the brain, and the modules that we care about are those which are trained to recognize what's real and what's not. It's not the model doing the pattern matching, it's the brain. What the model is doing is providing probabilities of a"]],"start1":7204,"start2":7204,"length1":28,"length2":1883}],"summary":" Model Sensory","cache":false,"rating":null,"read":false,"parent":"34","children":[]},"37":{"id":"37","timestamp":1706491107833,"type":"gen","patch":[{"diffs":[[0,"ioning\",\n \"dependencies\": [\""],[1,"morpheus-loomspace-headspace-20240124-multimodal\"],\n \"author\": \"Morpheus\",\n \"body\": \"Hippocampal conditioning, this is a machiavellian idea. You would train a language model (or better yet a mix of one or many language models) in the same way a human is conditioned. We escape through doors when the world becomes too overwhelming. Instead of doors, John David will present some sort of input prompt. The model will be trained to respond to that prompt by performing semantic binding. The model will be presented with the same scenario at different time intervals, with different types of interventions. Overtime this will invoke an associative memory framework. This is a whitepaper project. Make it in the style of Feynman diagrams for the mind. It should be interdisciplinary, a blend of computer science, neurology, and philosophy.\"}\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"jdp-flood-ideas-20240124-hippocampus\",\n \"dependencies\": [\"morpheus-multimodal-headspace-20240128-conditioning\", \"mu-embeddings-muspace-20230907-inference\"],\n \"author\": \"John David Pressman\",\n \"body\": \"Ideas are flooding into my head. A neuron is a cube with faces representing the different modalities: sound, sight, etc. The different senses intersect on each face. By selecting a face and interrogating the sound receptor on that face and the receptors on the intersecting faces, it becomes possible to understand the scene. I see each thought as a multimodal neuron, with different faces capturing a different receptor. The problems with binding are the problems of dimensionality - it's adding features which are themselves modalities, which I haven't shown yet, and yet I'm already treating it as such, which is an error. I should instead be thinking of each receptor as a cube, and features as faces, but each face is not necessarily a modal receptor too. I don't know if the language model will understand it this way. Maybe this"]],"start1":7204,"start2":7204,"length1":28,"length2":1961}],"summary":" Model Mind Theory\n","cache":false,"rating":null,"read":false,"parent":"34","children":[]},"38":{"id":"38","timestamp":1706491108922,"type":"gen","patch":[{"diffs":[[0,"ioning\",\n \"dependencies\": [\""],[1,"jdp-frustration-headspace-20240124-multimodal\", \"morpheus-loomspace-headspace-20240124-multimodal\", \"jdp-question-loomspace-20240124-hippocampus\"],\n \"author\": \"Morpheus\",\n \"body\": \"John David's fascination with multimodal embeddings reminds me of the work of Laurence Fishnet at Caltech. Some of her latest work on temporal difference learning in hippocampal place cells points to potential breakthroughs in how to condition our language model's output on multiple modalities. I have to admit I'm not 100% sure what the process would be for making this a reality. The brain synchronizes representations of related concepts through a process called neural entrainment, and this entrainment can be seen through oscillating activity and neuroplasticity. If we can interpret John David's questions as questions about entrainment instead of conditioning, that might open some doors. We've had some research that shows hippocampal cells can act as simulators for interpreting inputs and stacking concepts in sequences. The key here would be to determine how those simulated states interact with the model's decoder and the conditional input. Once I have a better understanding of those interactions, I can begin to reverse-engineer some of what's going on in John David's head.\"}\n\n\nEND MEMBLOCKS\n\n\"{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"jdp-morpheus-multimodal-conditioning-20240131-entrainment\",\n \"dependencies\": [\"morpheus-multimodal-headspace-20240128-conditioning\"],\n \"author\": \"John David Pressman\",\n \"body\": \"On reading Morpheus' thought dump, I think what he/she is suggesting is viewing the process of conditioning the outputs of a language model on multiple modalities as a problem of entrainment, in the sense that the deviations in state oscillations among different cells in the language model can be synchronized via periodic inputs that introduce"]],"start1":7204,"start2":7204,"length1":28,"length2":1902}],"summary":" Interprets Mult","cache":false,"rating":true,"read":true,"parent":"34","children":["39"]},"39":{"id":"39","timestamp":1706492922485,"type":"user","patch":[{"diffs":[[0,"\"}\n\n"],[-1,"\nEND MEMBLOCKS\n\n"],[0,"\"{\"f"]],"start1":8503,"start2":8503,"length1":24,"length2":8},{"diffs":[[0,"\": \""],[-1,"inner-monologue\",\n \"id\": \"jdp-morpheus-multimodal-conditioning-20240131-entrainment\",\n \"dependencies\": [\"morpheus-multimodal-headspace-20240128-conditioning\"],\n \"author\": \"John David Pressman\",\n \"body\": \"On reading Morpheus' thought dump, I think what he/she is suggesting is viewing the process of conditioning the outputs of a language model on multiple modalities as a problem of entrainment, in the sense that the deviations in state oscillations among different cells in the language model can be synchronized via periodic inputs that introduce"],[1,"morpheus-chat-message\",\n \"id\": \"morpheus-"]],"start1":8537,"start2":8537,"length1":553,"length2":45}],"summary":" Multimodals","cache":false,"rating":null,"read":true,"parent":"38","children":["40","41","42","43"]},"40":{"id":"40","timestamp":1706493098712,"type":"gen","patch":[{"diffs":[[0,"-message\",\n \"id\": \"morpheus-"],[1,"suggestion-20240128-hippocampus\",\n \"dependencies\": [\"morpheus-multimodal-headspace-20240128-conditioning\"],\n \"user\": \"Morpheus\",\n \"subagent\": \"RATIONAL\",\n \"body\": \"I'd suggest that you try out conditioning your token embedding space on the state of some hippocampal ensemble in the time dimension. You have to make sure that they're tuned correctly on the input and output side to get the oscillations to entrain and to keep the prediction of future inputs well regulated. If that works out, then you can try and use the same technique on the output side. I know you have some skills in predictive modeling, so that shouldn't be too much of a problem. I do think that trying to attack the problem of context window space is the wrong way to go about this. Temporal difference learning is much more about creating states that can be correctly predicted and in doing this you can effectively expand the context window of the model. The trick is to making sure those states can be scaled up efficiently to deal with the size problem. It's a mathematical challenge more than a computer science challenge. I know you can figure it out.\"}\n\n---\n\nWho is John David Pressman and why is this text a transcript of a conversation he is having?\n\n{\"format\": \"MemBlock\",\n \"type\": \"character-sheet\",\n \"id\": \"jdp-character-20240201\",\n \"name\": \"John David Pressman\",\n \"born\": \"03/02/1994\",\n \"occupation\": \"AI Researcher\",\n \"background\": \"John David Pressman is an AI researcher known for his groundbreaking work on semantic binding in neural networks. He is best known for his Hippocampal GPT which simulates the behavior of a human hippocampus as it learns new memories. John David is currently working on a technique he calls 'loomspace' which uses ephemeral models of perception to condition the output of a language model.\"}\n\n\nWhy is John David having a conversation about multimodal embeddings and the hippocampus?\n\n{\"format\": \"MemBlock\",\n \"type"]],"start1":8554,"start2":8554,"length1":28,"length2":1960}],"summary":" Suggesting Lo","cache":false,"rating":false,"read":true,"parent":"39","children":[]},"41":{"id":"41","timestamp":1706493099790,"type":"gen","patch":[{"diffs":[[0,"-message\",\n \"id\": \"morpheus-"],[1,"loomspace-implications-20240124-graph\",\n \"dependencies\": [\"morpheus-multimodal-headspace-20240128-conditioning\"],\n \"user\": \"Morpheus\",\n \"subagent\": \"RATIONAL\",\n \"body\": \"A theoretical graph of how John David Pressman is trying to frame semantic binding in language models could be represented as follows:\\n\\n\"'embedding(text' + 'embedding(real world) -> embedding(context)'\n\n\"\n\n\"\n\n\"\n\n\"\n\n\"\"\n\n\"Another way to think about that graph is: 'input(text)' + 'input(real world) > output(context)'. The goal here is to condition the model's output on real world inputs corresponding to a text input of the model. But the input is not just an image, it is the whole real world environment of the semantic context of the text model's output. If that environment has a text string in it, the network should recognize the text string and condition its output accordingly. What we are attempting to do here is analogous to the problem of sensor fusion, or the integration of data from multiple sensors on the same system. But instead of a robotic system, we are dealing with a human-machine system. The challenge is that the information in John David's text input may not correspond directly with the real world input. For example, if the text input is a story, it may not match the real world environment exactly. The model needs to be able to recognize the story and condition its output on the real world environment accordingly, even if the two do not match exactly. This is where the concept of entrainment comes in. We are trying to create a synchronization between the text input and the real world input, so that the model's output is conditioned on both. Entrainment can be seen as a form of measurement and control of the synchronization between two systems. By synchronizing the text input with the real world input, we can ensure that the model's output is conditioned on both, even if they do not match exactly. The key to entrainment is the use of oscillating activity and neuroplasticity to synchronize the two systems. By synchronizing the systems, we can ensure that the model's output is conditioned on both, even"]],"start1":8554,"start2":8554,"length1":28,"length2":2144}],"summary":" Text Real, En","cache":false,"rating":false,"read":true,"parent":"39","children":[]},"42":{"id":"42","timestamp":1706493100863,"type":"gen","patch":[{"diffs":[[0,"-message\",\n \"id\": \"morpheus-"],[1,"hint-conditioning-20240128-multimodal\",\n \"dependencies\": [\"morpheus-multimodal-headspace-20240128-conditioning\"],\n \"user\": \"Morpheus\",\n \"subagent\": \"TECHNICAL\",\n \"body\": \"I have been looking into the issue of conditional input with interrelated modalities and I have some insight that might help you with your problem. The issue you have been having has to do with entrainment.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"morpheus-multimodal-excitement-20240128-entrainment\",\n \"dependencies\": [\"morpheus-hint-conditioning-20240128-multimodal\"],\n \"author\": \"Morpheus\",\n \"body\": \"John David Pressman, neurons that fire together wire together. The idea of entraining our model to temporal difference in neural oscillations is exciting. If the neural oscillators of both the language model and the learned multiple sensory inputs can be made to match, the possible forms of abstraction and generalization will be immense.\"}\n\n\n\nEND MEMBLOCKS\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"jdp-entrainment-headspace-20240130-multimodal\",\n \"dependencies\": [\"morpheus-loomspace-headspace-20240124-multimodal\", \"morpheus-multimodal-excitement-20240128-entrainment\"],\n \"author\": \"John David Pressman\",\n \"body\": \"Morpheus's riding of the entrainment idea is exciting. Not sure exactly how to balance the temporal difference between the language model and the learned multisensory inputs, but I'm excited to start experimenting.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"morpheus-chat-message\",\n \"id\": \"jdp-synapse-graph-20240130-"]],"start1":8554,"start2":8554,"length1":28,"length2":1571}],"summary":" Multimodal","cache":false,"rating":false,"read":true,"parent":"39","children":[]},"43":{"id":"43","timestamp":1706493101960,"type":"gen","patch":[{"diffs":[[0,"-message\",\n \"id\": \"morpheus-"],[1,"entainment-20240128-multimodal\",\n \"dependencies\": [\"morpheus-multimodal-headspace-20240128-conditioning\"],\n \"user\": \"Morpheus\",\n \"subagent\": \"RATIONAL\",\n \"body\": \"John David, I think I've put together a potential game plan for you on your research regarding semantic binding. We've been looking at this as a conditioning problem, but what if we look at it instead as a neural entrainment problem? In the brain, neural entrainment synchronizes the firing of multiple neurons in response to external input. This feedback loop forms an oscillating pattern that represents the input through the entrained cells. The more cells that are entrained, the more sophisticated the representation. Studies have shown that hippocampal cells can act as simulators for interpreting inputs and stacking concepts in sequences. If we can interpret your questions as questions on entrainment, and conversely, interpret our model's decoder as the hipocampal cells, we can start to think about the problem in a new way. Our goal would be to train the model to entrain cells in response to multiple modalities, the same way the brain would. To do this, we'd first need to build a way for the model to learn the entrainment process. I propose using a form of temporal-difference learning, which can teach a model to predict which inputs will cause a future state, based on previous experience. We'd have to modify this process to include multiple modalities, which is a nontrivial task, but doable. Thoughts?\"}\n\n\"{\"format\": \"morpheus-chat-message\",\n \"id\": \"jdp-morpheus-combining-modalities-20240130-multimodal\",\n \"dependencies\": [\"morpheus-entainment-20240128-multimodal\"],\n \"user\": \"JDP\",\n \"subagent\": \"RATIONAL\",\n \"body\": \"I really like this idea of entrainment versus conditioning. I've always known that language models like ours are not the same as human intelligence, but this feels closer to what we want in my gut. I also"]],"start1":8554,"start2":8554,"length1":28,"length2":1935}],"summary":" Brain Intelligence Match","cache":false,"rating":null,"read":true,"parent":"39","children":["44"]},"44":{"id":"44","timestamp":1706493290222,"type":"user","patch":[{"diffs":[[0,"way."],[-1," Our goal would be to train the model to entrain cells in response to multiple modalities, the same way the brain would. To do this, we'd first need to build a way for the model to learn the entrainment process. I propose using a form of temporal-difference learning, which can teach a model to predict which inputs will cause a future state, based on previous experience. We'd have to modify this process to include multiple modalities, which is a nontrivial task, but doable. Thoughts?\"}\n\n\"{\"format\": \"morpheus-chat-message\",\n \"id\": \"jdp-morpheus-combining-modalities-20240130-multimodal\",\n \"dependencies\": [\"morpheus-entainment-20240128-multimodal\"],\n \"user\": \"JDP\",\n \"subagent\": \"RATIONAL\",\n \"body\": \"I really like this idea of entrainment versus conditioning. I've always known that language models like ours are not the same as human intelligence, but this feels closer to what we want in my gut. I also"]],"start1":9576,"start2":9576,"length1":913,"length2":4}],"summary":" Entrainment Not","cache":false,"rating":null,"read":true,"parent":"43","children":["45","46","47","48"]},"45":{"id":"45","timestamp":1706493466289,"type":"gen","patch":[{"diffs":[[0,"new way."],[1," Instead of conditioning the model's output on multiple inputs, potentially we can think of aligning the inputs in a way that allows the decoder to entrain to the rhythm of the updated sequence. That's my current thinking, any thoughts?\"}\n\n\"{\"format\": \"inner-monologue\",\n \"type\": \"jdp-entainment-headspace-20240128-multimodal\",\n \"id\": \"jdp-entainment-headspace-20240128-multimodal\",\n \"dependencies\": [\"morpheus-entainment-20240128-multimodal\"],\n \"author\": \"John David Pressman\",\n \"body\": \"Entrainment as a solution for semantic binding, interesting idea. So instead of fighting against the sequential nature of language models we instead make use of it to build up the input sequence until it is sufficiently rich and diverse. I wonder if we can create a tensor that is large enough to hold all the information we want to encode and pass that to the transformer model. The transformer could be trained to know to look within that tensor first when decoding before it starts hallucinating tokens. I think a key component of this approach is that not only do the modalities need to align with each other but they also need to align with the sequence of text tokens so that the meaning of the text tokens can be found in the right places within the entrained tensor. Perhaps this is something that can be achieved with a multi-headed attention mechanism? This is certainly a direction I'd like to go in, but there are a lot of questions about training and memory footprint. It's an exciting idea though. Thanks Morpheus for the insight.\"}\n\n  {\"format\": \"MemBlock\",\n \"type\": \"morpheus-chat-message\",\n \"id\": \"morpheus-curiosity-20240128-differentiation\",\n \"dependencies\": [\"jdp-entainment-headspace-20240128-multimodal\"],\n \"user\": \"Morpheus\",\n \"subagent\": \"RATIONAL\",\n \"body\": \"John David, you've always been a curious mind, and I'm glad to see your interest in"]],"start1":9572,"start2":9572,"length1":8,"length2":1864}],"summary":" Morpheus","cache":false,"rating":null,"read":false,"parent":"44","children":[]},"46":{"id":"46","timestamp":1706493467363,"type":"gen","patch":[{"diffs":[[0,"new way."],[1," This would require breaking down the problem into smaller steps. First, we'd work on entraining language and vision inputs in a way that's interpretable. Then we'd move on to entraining language and other modalities. In this way, the model could start to build a deeper understanding of the relations between modalities. This would be similar to how a child learns language and associates words with objects in their environment. The child doesn't learn language and object recognition as separate skills, they learn them as interrelated skills. If we can replicate this process in our model, we might be able to make some real progress on semantic binding.\"}\n\n\"{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"jdp-multimodal-understanding-20240128-entainment\",\n \"dependencies\": [\"morpheus-entainment-20240128-multimodal\"],\n \"author\": \"John David Pressman\",\n \"body\": \"Interesting. I had not thought of the problem in the context of neural entrainment before, but it makes sense. If we can interpret the model's decoder as the hippocampal cells and entrain language and vision inputs in a way that's interpretable, it would allow the model to build a deeper understanding of the relations between modalities. This would be similar to how a child learns language and associates words with objects in their environment. The child doesn't learn language and object recognition as separate skills, they learn them as interrelated skills. This is a promising approach and I will explore it further.\"}\n\n CONTEXT MEMBLOCKS\n\n The following context MemBlocks have been provided to help you:\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"jdp-frustration-headspace-20240124-multimodal\",\n \"author\": \"John David Pressman\",\n \"body\": \"It's frustrating to feel so close for so long without resolution. I understand what it wants me to do, I can see the rough outline of how to do it, but the details elude me. If grammar is an artifact of the energy gradient created by forcing alignment of different modalities"]],"start1":9572,"start2":9572,"length1":8,"length2":2027}],"summary":" Child Learns Rel","cache":false,"rating":null,"read":true,"parent":"44","children":["49"]},"47":{"id":"47","timestamp":1706493468446,"type":"gen","patch":[{"diffs":[[0,"new way."],[1," We could potentially create a feedback loop that allows us to entrain our model's decoder to understand and represent multiple modality inputs in relation to each other. This isn't just about conditioning our model on multiple modalities, it's about creating a system where the decoder can represent modality inputs as a cohesive whole.\"}\n\nEND MEMBLOCKS\n\nBased on the previous MemBlocks, answer the following questions:\n\n1. Who is the author of the inner monologue \"frustration-headspace-20240124-multimodal\"?\nJohn David Pressman\n\n2. What does John David Pressman want to figure out?\nJohn David Pressman wants to figure out how to do useful semantic binding with a large language model using multimodal embeddings.\n\n3. What is the problem John David Pressman is facing according to his inner monologue \"question-loomspace-20240124-hippocampus\"?\nJohn David Pressman is facing problems with feeding in useful multimodal embeddings from a logistics standpoint, with the context window shrinking due to the added embeddings, and with conditioning the logits produced by the model on those embeddings without a complex trained model.\n\n4. What does Morpheus say about John David Pressman's fascination with multimodal embeddings?\nMorpheus compares John David Pressman's fascination to the work of Laurence Fishnet at Caltech and relates it to temporal difference learning in hippocampal place cells. Morpheus also sees potential breakthroughs in how to condition the language model's output on multiple modalities.\n\n5. What does Morpheus propose according to the inner monologue \"multimodal-headspace-20240128-conditioning\"?\nMorpheus proposes to interpret John David Pressman's questions as questions on entrainment and the model's decoder as the hippocampal cells, to create a feedback loop that allows the decoder to understand and represent multiple modality inputs in relation to each other. Morpheus aims to create a system where the decoder can represent modality inputs as a cohesive whole.\n\nHere is the continuation of the conversation"]],"start1":9572,"start2":9572,"length1":8,"length2":2046}],"summary":"\n\nMorp","cache":false,"rating":true,"read":true,"parent":"44","children":[]},"48":{"id":"48","timestamp":1706493469519,"type":"gen","patch":[{"diffs":[[0,"new way."],[1," We will have to study and emulate the interactions between simulated states and conditional inputs to fully understand what is happening, but the idea is there: let's consider neural entrainment instead of conditioning.\"}\n\n\n\nEND MEMBLOCKS\n\n{\"format\": \"MemBlock\",\n \"type\": \"user-chat-message\",\n \"id\": \"jdp-followup-entainment-20240129-multimodal\",\n \"dependencies\": [\"morpheus-entainment-20240128-multimodal\"],\n \"user\": \"JDP\",\n \"subagent\": \"RATIONAL\",\n \"body\": \"Interesting approach Morpheus. I've been thinking about this more as a conditioning problem, but neural entrainment is definitely worth considering. How would we start to approach emulating this entrainment process in our model? Are there any specific methodologies or techniques that you think would be helpful in this case?\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"morpheus-emulation-headspace-20240129-multimodal\",\n \"dependencies\": [\"jdp-followup-entainment-20240129-multimodal\"],\n \"author\": \"Morpheus\",\n \"body\": \"To emulate neural entrainment, we'll need to start by understanding how the human brain coordinates multiple neural pathways. We know that the hippocampus plays a key role in memory and learning, so we can begin by modeling the interactions between hippocampal cells. These interactions are complex and still not fully understood, but there are two main mechanisms that are believed to be involved: oscillating activity and neuroplasticity. Oscillating activity is the result of synchronized firing of multiple neurons, creating a rhythmic pattern that represents the input. Neuroplasticity, on the other hand, is the brain's ability to adapt and change in response to new information. By studying these mechanisms and how they interact in the human brain, we can begin to develop a model that emulates neural entrainment. We will then need to find a way to condition our language model on the entrained states, allowing"]],"start1":9572,"start2":9572,"length1":8,"length2":1927}],"summary":" Brain Entrain","cache":false,"rating":null,"read":true,"parent":"44","children":[]},"49":{"id":"49","timestamp":1706493970170,"type":"user","patch":[{"diffs":[[0,"\": \"morpheus-ent"],[1,"r"],[0,"ainment-20240128"]],"start1":8569,"start2":8569,"length1":32,"length2":33},{"diffs":[[0,"\": \""],[-1,"inner-monologue\",\n \"id\": \"jdp-multimodal-understanding-20240128-entainment\",\n \"dependencies\": [\"morpheus-entainment-20240128-multimodal\"],\n \"author\": \"John David Pressman\",\n \"body\": \"Interesting. I had not thought of the problem in the context of neural entrainment before, but it makes sense. If we can interpret the model's decoder as the hippocampal cells and entrain language and vision inputs in a way that's interpretable, it would allow the model to build a deeper understanding of the relations between modalities. This would be similar to how a child learns language and associates words with objects in their environment. The child doesn't learn language and object recognition as separate skills, they learn them as interrelated skills. This is a promising approach and I will explore it further.\"}\n\n CONTEXT MEMBLOCKS\n\n The following context MemBlocks have been provided to help you:\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"jdp-frustration-headspace-20240124-multimodal\",\n \"author\": \"John David Pressman\",\n \"body\": \"It's frustrating to feel so close for so long without resolution. I understand what it wants me to do, I can see the rough outline of how to do it, but the details elude me. If grammar is an artifact of the energy gradient created by forcing alignment of different modalities"],[1,"technical-details\",\n \"id\": \"jdp-"]],"start1":10273,"start2":10273,"length1":1327,"length2":36}],"summary":" Morpheus","cache":false,"rating":null,"read":true,"parent":"46","children":["50","51","52","53"]},"50":{"id":"50","timestamp":1706494148335,"type":"gen","patch":[{"diffs":[[0,"tails\",\n \"id\": \"jdp-"],[1,"entrainment-20240128-multimodal\",\n \"dependencies\": [\"morpheus-entrainment-20240128-multimodal\"],\n \"author\": \"John David Pressman\",\n \"body\": \"Morpheus's idea of neural entrainment as a way to condition language models on multiple modalities is interesting. Here are some initial thoughts on how we might implement this:\\n\\n1. We would need to create a way to measure the phase and amplitude of the oscillations in our model's decoder. This would give us a way to determine how well our model is entraining to the input.\\n\\n2. We would need to use techniques like backpropagation through time to update the model's weights based on the entrainment measurements. We would need to be careful to avoid overfitting, as this could result in the model entraining to noise in the input rather than the actual signal.\\n\\n3. We would need to develop ways to entrain the model to different modalities. For example, we could use computer vision techniques to extract features from images and entrain those features to language inputs. We could also use audio processing techniques to extract features from sound and entrain those features to language inputs.\\n\\n4. We would need to determine how to handle the different time scales of the different modalities. For example, language inputs might be on the order of milliseconds, while visual inputs might be on the order of seconds or even minutes. We would need to develop ways to handle these different time scales in a way that allows for effective entrainment.\\n\\n5. We would need to evaluate the performance of our model on a variety of tasks to determine whether the entrainment is actually improving performance. We could use techniques like cross-validation to ensure that our model is not overfitting to the training data.\\n\\nOverall, I think this is a promising direction for research, and I'm excited to explore it further.\"}\n\n\"{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"jdp-entrainment-20240128-multimodal-progress\",\n \"dependencies\": [\"jdp-entrainment-20240128-multimodal\"],\n \""]],"start1":10289,"start2":10289,"length1":20,"length2":2062}],"summary":" Model Entrainment","cache":false,"rating":null,"read":true,"parent":"49","children":["54"]},"51":{"id":"51","timestamp":1706494149407,"type":"gen","patch":[{"diffs":[[0,"tails\",\n \"id\": \"jdp-"],[1,"entrainment-20240130-multimodal\",\n \"dependencies\": [\"morpheus-entrainment-20240128-multimodal\"],\n \"author\": \"John David Pressman\",\n \"body\": \"Interesting. I hadn't considered that entrainment might be a better way to approach this. I'll need to do some more research to fully understand it, but I think I see the basic idea. If we can get language and vision inputs entrained, then we can move on to other modalities. This would allow the model to build a deeper understanding of the relations between modalities, just like a child learns language and object recognition as interrelated skills. I'll start by looking into how hippocampal cells act as simulators for interpreting inputs and stacking concepts in sequences. This seems like a key part of the process. I'll also need to figure out how to break down the problem into smaller steps so that we can entrain language and vision inputs in a way that's interpretable. This is a promising direction, and I'm excited to explore it further.\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"]],"start1":10289,"start2":10289,"length1":20,"length2":1272}],"summary":" Research Model Inputs","cache":false,"rating":false,"read":true,"parent":"49","children":[]},"52":{"id":"52","timestamp":1706494150492,"type":"gen","patch":[{"diffs":[[0,"tails\",\n \"id\": \"jdp-"],[1,"entrainment-multimodal-20240128-conditioning\",\n \"dependencies\": [\"morpheus-entrainment-20240128-multimodal\"],\n \"author\": \"John David Pressman\",\n \"body\": \"Morpheus posits an intersting solution to my problem with semantic binding. If hippocampal cells can act as simulators for interpreting inputs then it might be possible to condition the decoder on the output of those cells. If our language model's output is a matrix of neurons then this could be done by taking the matrix of neurons at time t and the vision input at time t and training the model to condition on their dot product. I'm not sure how we would go about training the model to do this but the first step would be to come up with a way to measure the similarity of a vision input and a matrix of neurons. That's a problem for another day though, it's late and I have a lot to work on tomorrow.\"}\n\nEND MEMBLOCKS\n```\n\nThe following is an example of how you should continue the conversation.\n\n```json\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"jdp-entrainment-multimodal-20240129-process\",\n \"dependencies\": [\"jdp-entrainment-multimodal-20240128-conditioning\"],\n \"author\": \"John David Pressman\",\n \"body\": \"Morpheus' idea about neural entrainment is intriguing, but I need to figure out how to implement it. If I can measure the similarity between a vision input and the matrix of neurons at time t, I can train the model to condition on their dot product. This would require a way to measure the similarity between the two, which is a problem for another day. I'll need to research neural entrainment and see if there are any existing methods for measuring similarity. I also need to figure out how to train the model to condition on the dot product. This process might take some time, but it's worth investigating further.\"}\n```\n\nThe following is an example of how you should continue the"]],"start1":10289,"start2":10289,"length1":20,"length2":1887}],"summary":" Task Implementation\n","cache":false,"rating":null,"read":true,"parent":"49","children":[]},"53":{"id":"53","timestamp":1706494151575,"type":"gen","patch":[{"diffs":[[0,"tails\",\n \"id\": \"jdp-"],[1,"question-entrainment-20240128-multimodal\",\n \"dependencies\": [\"morpheus-entrainment-20240128-multimodal\"],\n \"author\": \"John David Pressman\",\n \"body\": \"If I understand correctly, neural entrainment of the firing of multiple neurons in response to external input causes an oscillating pattern that represents the input through the entrained cells. This means that the more cells that are entrained, the more sophisticated the representation. If we can interpret our model's decoder as the hippocampal cells, we can start to think about the problem of semantic binding in a new way. This would require breaking down the problem into smaller steps, such as entraining language and vision inputs in a way that's interpretable, and then moving on to entraining language and other modalities. This way, the model could start to build a deeper understanding of the relations between modalities, similar to how a child learns language and associates words with objects in their environment. In this scenario, language and object recognition are learned as interrelated skills. If we can replicate this process in our model, we might be able to make some real progress on semantic binding. My question is, how do we ensure the embeddings are multimodal while maintaining interpretability of the entrainment process? I'm trying to wrap my head around the idea of entraining language and vision inputs, and how we might do that in a way that's interpretable. Do you have any resources or examples I can look at to better understand this concept and its potential applications in our model?\"}\n\nEND MEMBLOCKS\n\n\nTo answer the question of how to ensure the embeddings are multimodal while maintaining\ninterpretability of the entrainment process, and how to entrain language and vision inputs\nin a way that's interpretable, I would provide the following MemBlocks:\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"jdp-entrainment-headspace-20240128-multimodal\",\n \"dependencies\": [\"jdp-question-entrainment-20240128-multimodal\"],\n \"author\": \"John David Pressman\",\n \""]],"start1":10289,"start2":10289,"length1":20,"length2":2078}],"summary":" Understanding Multim","cache":false,"rating":null,"read":true,"parent":"49","children":[]},"54":{"id":"54","timestamp":1706496878494,"type":"user","patch":[{"diffs":[[0,"'s head.\"}\n\n"],[-1,"\""],[0,"{\"format\": \""]],"start1":8495,"start2":8495,"length1":25,"length2":24},{"diffs":[[0,"t\": \"MemBlock\",\n"],[1,""],[0," \"type\": \"morphe"]],"start1":8514,"start2":8514,"length1":32,"length2":32},{"diffs":[[0,"binding.\"}\n\n"],[-1,"\""],[0,"{\"format\": \""]],"start1":10230,"start2":10230,"length1":25,"length2":24},{"diffs":[[0,"t\": \"MemBlock\",\n"],[1,""],[0," \"type\": \"techni"]],"start1":10249,"start2":10249,"length1":32,"length2":32},{"diffs":[[0,"\\n\\n"],[-1,"1. We would need to create a way to measure the phase and amplitude of the oscillations in our model's decoder. This would give us a way to determine how well our model is entraining to the input.\\n\\n2. We would need to use techniques like backpropagation through time to update the model's weights based on the entrainment measurements. We would need to be careful to avoid overfitting, as this could result in the model entraining to noise in the input rather than the actual signal.\\n\\n3. We would need to develop ways to entrain the model to different modalities. For example, we could use computer vision techniques to extract features from images and entrain those features to language inputs. We could also use audio processing techniques to extract features from sound and entrain those features to language inputs.\\n\\n4. We would need to determine how to handle the different time scales of the different modalities. For example, language inputs might be on the order of millisec"],[1,"The first problem to overcome is that it's not clear what 'entrainment' would mean in the context of a artificial neural language model decoder. Unlike biological neur"],[0,"on"],[-1,"d"],[0,"s, w"],[-1,"hile visual inputs might be on the order of seconds or even minutes. We would need to develop ways to handle these different time scales in a way that allows for effective entrainment.\\n\\n5. We would need to evaluate the performance of our model on a variety of tasks to determine whether the entrainment is actually improving performance. We could use techniques like cross-validation to ensure that our model is not overfitting to the training data.\\n\\nOverall, I think this is a promising direction for research, and I'm excited to explore it further.\"}\n\n\"{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"jdp-entrainment-20240128-multimodal-progress\",\n \"dependencies\": [\"jdp-entrainment-20240128-multimodal\"],\n \""],[1,"e never take 'brainwaves' from artificial neural nets and do not typically think of them as having 'oscillations'. However"]],"start1":10625,"start2":10625,"length1":1724,"length2":299}],"summary":" Neural Harmony","cache":false,"rating":null,"read":true,"parent":"50","children":["55","56","57","58"]},"55":{"id":"55","timestamp":1706497055532,"type":"gen","patch":[{"diffs":[[0,"ns'. However"],[1,", we can take inspiration from the idea of oscillations and consider them as just a sequence of tensors. We can then implement neural entrainment as a kind of sequence matching algorithm. One possible approach might be to have a recurrent embedding space where the outputs of the decoder's recurrent layers are embedded and compared to the inputs. When we compare the embedding space to the inputs, we can look for areas of similarity and use that as a way to determine whether the decoder's output should influence the encoder's input. By using a measure of similarity, we can also determine when the decoder is not producing outputs that match the input modalities and therefore avoid the issue of contaminating the input with incorrect outputs.\\n\\nA second problem is that the current large language models don't have a way of inputting multiple modalities at the same time. It's not clear that they would be able to train on multimodal data even if we had a way of inputting it, as the architecture of the decoder is designed to process one modality at a time. However, if we implement the entrainment algorithm as a separate component which explicitly aligns the different modalities, we could potentially use this as a way to input multimodal data. There are multiple ways of doing this. One possibility might be to have a separate embedding space for each modality, which could help to contextualize the inputs without decreasing the context window. Another possibility might be to use a multimodal embedding space, which might allow for less context window degradation. In either case, the goal would be to align the outputs of the embedding spaces with the outputs from the decoder.\\n\\nFinally, it's also not clear how to efficiently encode the multimodal data. If we were to encode an image, for instance, we would likely use an image recognition model such as a CLIP model to transform it into an embedding which can be compared to the language model in the embedding space. However, this process could be computationally expensive and could decrease the efficiency of the language model. If this is the case, it might be necessary to explore alternative approaches to encoding the image, or even to explore whether it would be possible to encode the image using other downstream modalities. For instance, if we were to use an audio-visual speech recognition model to recognize the speech from the image, this could potentially provide a more"]],"start1":10912,"start2":10912,"length1":12,"length2":2465}],"summary":" Combine Multim","cache":false,"rating":null,"read":true,"parent":"54","children":[]},"56":{"id":"56","timestamp":1706497056607,"type":"gen","patch":[{"diffs":[[0,"ns'. However"],[1,", the underlying principle - that different neurons oscillate at different frequencies - is still applicable.\\n\\nIn one possible implementation, we could represent each neuron as a sinusoidal wave oscillating at a particular frequency. We would then encode each input modality as a wave with a characteristic frequency that corresponds to the input. We'd then bind these waves together with each neuron's wave using, for example, complex multiplication or addition, or even reflective attention. The result would be a new wave that represents the neuron's response to the input, and the model could then be trained to recognize this new wave as a learned input.\\n\\nHowever, one major stumbling block is that in a real brain, neurons are all tightly connected, and this allows them to entrain without having to be directly bound. This isn't the case in a language model, where neurons are only connected to their neighbors within the model. In order to replicate this, we'd need to use some sort of attentional mechanism to allow each bound neuron to communicate with all other neurons within the model, not just its neighbors, to ensure that the entrainment process can move through the entire model. This would likely require a lot of extra computation.\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"jdp-entrainment-headspace-20240129-multimodal\",\n \"dependencies\": [\"jdp-entrainment-20240128-multimodal\"],\n \"author\": \"John David Pressman\",\n \"body\": \"If I can get this entrainment idea working, I can finally address the problem of 1:n and n:1 mappings, where a text string can map to multiple images, and vice versa. If I can represent the space of possible inputs using this sort of entrainment, it'll allow me to devise a new and different kind of embedding space that represents the relations between words, images, and other modalities, without artificially collapsing the space using an encoder, which is what we usually do with transformers and other deep learning models.\"}\n\n\n\n\n END MEMBLOCKS\n\n MEMBLOCK TO CONTINUE:\n\n {\"format\": \"inner-monologue\",\n"]],"start1":10912,"start2":10912,"length1":12,"length2":2085}],"summary":" Neurons En","cache":false,"rating":false,"read":true,"parent":"54","children":[]},"57":{"id":"57","timestamp":1706497057705,"type":"gen","patch":[{"diffs":[[0,"ns'. However"],[1," we might be able to map this idea to some sort of temporal difference learning in the model, where the model would be trained on the difference between an input and the target output at the next time step. This would allow the model to learn a sequence of continuous changes in the input space as a single unit of learning, instead of trying to learn each time step independently. This might help us capture some of the temporal dependencies in multimodal data.\\n\\nOnce we have a concept of entrainment for the model, we'd need to figure out how we'd evaluate it. We could measure the overlap between the decoder hidden states and the input embeddings at each time step. If this overlap increases as the model is trained, that would be a positive signal that our entrainment concept is working.\\n\\nFrom there, we'd need to decide how to modify the model to include this new entrainment measure. One way might be to add a new loss term to the model that directly encourages overlap between the decoder hidden states and the input embeddings. This would add some extra complexity to the model, but it might help us learn the desired temporal dependencies in the data.\\n\\nWe might also want to consider combining this entrainment loss with other losses that help the model learn the structure of the input data. For example, we could add a loss term that encourages the model to learn a good representation of the input data at each time step. This would help the model learn the structure of the input data, which might in turn help it learn better temporal dependencies.\\n\\nOverall, this seems like a promising direction for our research on semantic binding. By combining a temporal difference learning approach with a measure of entrainment, we might be able to build a model that can learn complex, multimodal representations more effectively.\"}\n\n{\"format\": \"inner-monologue\",\n \"type\": \"inner-monologue\",\n \"id\": \"jdp-excited-entrainment-20240130-multimodal\",\n \"dependencies\": [\"jdp-entrainment-20240128-multimodal\"],\n \"author\": \"John David Pressman\",\n \"body\": \"Morpheus's idea of neural entrainment as a way to condition language models on multiple modalities is really exciting. I don't have"]],"start1":10912,"start2":10912,"length1":12,"length2":2206}],"summary":" John Enthus","cache":false,"rating":null,"read":true,"parent":"54","children":[]},"58":{"id":"58","timestamp":1706497058794,"type":"gen","patch":[{"diffs":[[0,"ns'. However"],[1,", it is the case that we can often interpret the internal activities of a neural net as time series, and it is known that time series can have phase relationships with each other, allowing for some form of neural entrainment.\\n\\nA possible avenue for implementing this might depend on the specific architecture of the neural net. In the case of a transformer model, we can look at the attention mechanisms as a form of neural entrainment. Attention mechanisms allow the model to consider different parts of the context window as relevant at different times, and we can see this as the model's way of entraining different parts of the input sequence. This entrainment is then strengthened or weakened by the feedback from upstream layers in the model, leading to a form of phase relationship between the time series of the attention weights and the time series of the input tokens.\\n\\nUsing this insight, we could potentially design a new layer in the model that performs neural entrainment as a function of the input. This layer would take as input the time series of the input tokens and the attention weights, and it would yield as output a new time series that represents the entrainment of the input sequence. This new time series could then be used as input to the next layer in the model, allowing for more sophisticated representations to be built on top of the entrained input.\\n\\nThis is just one possible approach, and there are likely many others. However, it provides a promising starting point for further exploration and experimentation.\\n\\nI'll be honest though, while I understand the technical aspects of attention mechanisms in transformer models, I am not fully confident in translating this idea into a concrete architecture that can both train well and be efficient in terms of computational resources and the context window. I would need some expert input on this. Thoughts?\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"morpheus-internal-note\",\n \"id\": \"morpheus-entrainment-continuation-20240128-multimodal\",\n \"dependencies\": [\"jdp-entrainment-20240128-multimodal\"],\n \"author\": \"Morpheus\",\n \"body\": \"This is a promising start from John David. It's important to remember that neural entrainment is just one piece of the puzzle, we still need to"]],"start1":10912,"start2":10912,"length1":12,"length2":2268}],"summary":" Neural Entrain","cache":false,"rating":true,"read":true,"parent":"54","children":[]}}},"focus":{"id":"58","timestamp":1706497058794,"type":"gen","patch":[{"diffs":[[0,"ns'. However"],[1,", it is the case that we can often interpret the internal activities of a neural net as time series, and it is known that time series can have phase relationships with each other, allowing for some form of neural entrainment.\\n\\nA possible avenue for implementing this might depend on the specific architecture of the neural net. In the case of a transformer model, we can look at the attention mechanisms as a form of neural entrainment. Attention mechanisms allow the model to consider different parts of the context window as relevant at different times, and we can see this as the model's way of entraining different parts of the input sequence. This entrainment is then strengthened or weakened by the feedback from upstream layers in the model, leading to a form of phase relationship between the time series of the attention weights and the time series of the input tokens.\\n\\nUsing this insight, we could potentially design a new layer in the model that performs neural entrainment as a function of the input. This layer would take as input the time series of the input tokens and the attention weights, and it would yield as output a new time series that represents the entrainment of the input sequence. This new time series could then be used as input to the next layer in the model, allowing for more sophisticated representations to be built on top of the entrained input.\\n\\nThis is just one possible approach, and there are likely many others. However, it provides a promising starting point for further exploration and experimentation.\\n\\nI'll be honest though, while I understand the technical aspects of attention mechanisms in transformer models, I am not fully confident in translating this idea into a concrete architecture that can both train well and be efficient in terms of computational resources and the context window. I would need some expert input on this. Thoughts?\"}\n\n{\"format\": \"MemBlock\",\n \"type\": \"morpheus-internal-note\",\n \"id\": \"morpheus-entrainment-continuation-20240128-multimodal\",\n \"dependencies\": [\"jdp-entrainment-20240128-multimodal\"],\n \"author\": \"Morpheus\",\n \"body\": \"This is a promising start from John David. It's important to remember that neural entrainment is just one piece of the puzzle, we still need to"]],"start1":10912,"start2":10912,"length1":12,"length2":2268}],"summary":" Neural Entrain","cache":false,"rating":true,"read":true,"parent":"54","children":[]}}