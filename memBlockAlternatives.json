{"loomTree":{"root":{"id":"1","timestamp":1707523226666,"type":"root","patch":"","summary":"Root Node","cache":false,"rating":null,"read":true,"parent":null,"children":["2"]},"nodeStore":{"1":{"id":"1","timestamp":1707523226666,"type":"root","patch":"","summary":"Root Node","cache":false,"rating":null,"read":true,"parent":null,"children":["2"]},"2":{"id":"2","timestamp":1707532529583,"type":"user","patch":[{"diffs":[[1,"When thinking about memory in language models, we can start by noting that the classic precision-recall tradeoff means no one form of recall or indexing is going to be sufficient. This is because when we index a piece of information the ability to guess the index trades off against uniqueness of the index. We can trivially observe this is true by considering that unless an index has some kind of regular structure its identifiers cannot be guessed zero shot and regular structure that uniquely identifies a document converges to guessing the whole information content of that document in the limit. In classical library and information science terms, to increase precision (reduce the number of irrelevant documents retrieved by making the identifier more specific) we must decrease recall (here reduce the number of documents we can reliably expect to recall when we need them). \n\nThis tradeoff isn't new to library science, take for example"]],"start1":0,"start2":0,"length1":0,"length2":945}],"summary":" Memory In Models","cache":false,"rating":null,"read":true,"parent":"1","children":["3","4","5","6","11"]},"3":{"id":"3","timestamp":1707532619286,"type":"gen","patch":[{"diffs":[[0," example"],[1," indeterminacy in mathematics where a function f(x) is overdetermined when it can be uniquely identified by several different x values. A very famous example is the Prisonner's dilemma, where we for instance use the \"s1,s2,s3,s4\" identifier space to do the overidentification and generate distinct topologies. What is special here, however, is the language model's ability to abstract from the identifiers and generate the text of the function and variables using a unique structuring of the text space. This is a unique feature of language models not found in other types of information retrieval systems. \n\nWe can easily note that regularity in this identifier space is not necessarily regularity in the text space. The indeterminacy tradeoff where x'1,x'2,x'3,x'4 (different identifiers) generate the same function in the text space is an example of this. The indeterminacy tradeoff in mathematics and material science manifests in a number of nonconstructive theorems in mathematics (such as the existence of a prime number between every two prime numbers, the existence of a base"]],"start1":937,"start2":937,"length1":8,"length2":1092}],"summary":" Indetermin","cache":false,"rating":false,"read":true,"parent":"2","children":[]},"4":{"id":"4","timestamp":1707532620251,"type":"gen","patch":[{"diffs":[[0," example"],[1," the Dewey decimal system. Imagine you are trying to index documents on mathematics typically. Documents are added and removed with a high frequency, with higher frequency for more recent documents. On the other hand, the documents might be used for many years after they are indexed. Now, a typical document in this space is going to be concerned with some relatively specific piece of mathematics. Thus, if you are not careful, then you might end up with rather poor recall, i.e. the document is not retrieved when you search by keyword or ID. On the other hand, you might want to have a reasonable amount of precision, i.e. that when you search by keyword or ID it is a relatively small number of relevant documents that you find. The Dewey decimal system was originally a solution to this problem. By taking a common unit of mathematics- the theorem- and associating with a number, we can form a hierarchical classification that can be recursively applied. By filling in a retrieval form with the parts of the Dewey number corresponding to our keywords, we can achieve good precision and good recall. \n\nThere's a lot more to say about this, but it seems to me the current approaches"]],"start1":937,"start2":937,"length1":8,"length2":1194}],"summary":" Dewey System","cache":false,"rating":null,"read":true,"parent":"2","children":[]},"5":{"id":"5","timestamp":1707532621229,"type":"gen","patch":[{"diffs":[[0," example"],[1," its almost-synonym “the cataloging problem”. In the past it has been the case that compromises like selecting a small number of highly specific fundamental indexes let you explicitly reduce the scope of what they might be applied to (e.g. the DDC and LCC). This is obviously suboptimal for “natural language cognition” scenarios and TLDR; we’re soon going to be able to eyeball details in our data which can be covered by a small number of fundamental indexes, and then structure the remaining indexes in a manner that it is also a model of human memory.\n\nThere are a lot of reasons why this is more plausible than it might seem at first glance, one of which starts with the observation that a lot of classical library science data structures like the Library of Congress Classification and Dewey Decimal Classification are hierarchical, up to a point at which they become largely arbitrary. \n\nThe question about the shape of the trees that form the backbone of these hierarchies is deeply important, and involves understanding what are the basic abstractions upon which the tree functions. These trees can be understood as the product of pattern completion. \n\nOnce we write down a function"]],"start1":937,"start2":937,"length1":8,"length2":1199}],"summary":" Trees Compre","cache":false,"rating":null,"read":true,"parent":"2","children":["7","8","9","10"]},"6":{"id":"6","timestamp":1707532622208,"type":"gen","patch":[{"diffs":[[0," example"],[1," an index term which is a word in the document. Good index terms, in a precision sense, are specific words that are relevant to a document. If a document is about Maryam Mirzakhani and the term we are indexing is her name, then saying \"Maryam Mirzakhani is a specific person and the document is about her\" is a good index term. If the document is about meditation in general (which seems to have some relationship with the poster's example), the term \"meditation\" is also a good index term. But neither index term tells us anything about bits of information that are not about Maryam Mirzakhani or meditation, even if they are present in the document. This is a kind of indexing that is very specific, in a sense it is even a full content address (as I recall the ELI5 post some linked mentioned), but it has poor recall. This is a familiar thing in library science, where you direct users to a specific document. \n\nIn the ELI5 thread, I even linked the standard library scientific code indexing approach that is used for my discipline (astrophysics). It uses a system like the one you are describing, and if you read about it you"]],"start1":937,"start2":937,"length1":8,"length2":1138}],"summary":" Indexing Imprec","cache":false,"rating":false,"read":true,"parent":"2","children":[]},"7":{"id":"7","timestamp":1707532760172,"type":"gen","patch":[{"diffs":[[0," down a function"],[1," for pattern completion, we are able to link traditional library classifications with the more descriptive statistical algorithms which we're accustomed to envisioning in the context of deep learning. (This function is essentially a tree-shaped function for the Abstraction Hierarchy or ACM Computing Classification System, both of which are top-down hierarchical representations of the space of possible generalizations/abstractions of knowledge.)\n\nEssentially, one can think that the locality of the computation is a result of the hope that the depth of the thought experiment is bounded. This is essentially the same hope that underlies the trees of traditional hierarchies, with the exception that they never admit the possibility of arbitrariness, instead the trees tend to slowly fray out into a large number of small branches in which further differentiation is a meaningful direction to differentiate, and a larger number where it is not.\n\nHowever, this is a silly way to proceed, as Rich Sutton argues [“Representations, Invariances, and Good Engineering Practices for Modeling Human Cognition”](https://www.coursera.org/lecture/machine-learning/representations-"]],"start1":2120,"start2":2120,"length1":16,"length2":1187}],"summary":" Hierarchical Al","cache":false,"rating":null,"read":true,"parent":"5","children":[]},"8":{"id":"8","timestamp":1707532761153,"type":"gen","patch":[{"diffs":[[0," down a function"],[1," for pattern completion and build it as a tree, it becomes possible to add specificity and flexibly-chosen fudging factors to the tree to create any tree we want. Ongoing research by FeiFei Li et al. is already providing guidelines on how to do this because it relates to how humans create categories. \n\nAfter that, we can tie the tree to the hierarchies being used for the Bayesian approaches that are the gold standard for neural retrieval, model our errors on the errors that humans make, and start to think about ways to do pattern completion that also take into account hyperlinks and structure in our data that doesn't fit into the hierarchies. \n\nIn the model I presented in the video, the tree based upon the LCC forms the backbone of the model of the retrieval capability of the brain. It is entirely layered on top of existing library science data structures, taking into account statistical properties of human errors, and isn't even attempting to estimate document boundaries. \n\nIn other words, you can run this test on an arbitrary segment of text, such as the text from a single Wikipedia article. You can also run this test on the entire Wikipedia and not"]],"start1":2120,"start2":2120,"length1":16,"length2":1185}],"summary":" Pattern Completion\n","cache":false,"rating":null,"read":false,"parent":"5","children":[]},"9":{"id":"9","timestamp":1707532762130,"type":"gen","patch":[{"diffs":[[0," down a function"],[1," that reflects the basic abstraction in the domain we're working on, and we've written it in language model form, we have a process that’ll tell you which nodes in the newly formed tree are contained by a node you've already generated.\n\n\tThis process is called pattern completion and it’s basically a fancy name for imputing the values of inputs to a model that’s been trained on a number of examples.\n\nFrom a classical information theoretic point of view this is called indexing in which the index is implicit rather than explicit. \n\nThe idea that we can treat indexing into this tree hierarchy as a model of human memory has to do with the way that the tree can still be described as a generative model, in tree form. And since it can be put into a tree form it is implicitly polytomous rather than dichotomous.\n\n-----------------------------\n\nThere's a lot of work left to be done, but I believe there's a high chance that the ML methods that will be used to start connecting such a tree to a neural network based language model are going to be familiar and accessible to many, many people, and therefore will be very likely to"]],"start1":2120,"start2":2120,"length1":16,"length2":1146}],"summary":" Model Tree Language\n","cache":false,"rating":null,"read":true,"parent":"5","children":[]},"10":{"id":"10","timestamp":1707532763119,"type":"gen","patch":[{"diffs":[[0," down a function"],[1," with a particular form, we can infer a family of functions from a single example too (and different families from different examples). In the same way, we can infer a family of well-defined concepts from areas of our memory space which store a rich abundance / a paucity of information about the necessary conceptual elements. \n\nConsider that classical cataloging requires a large amount of time and effort, but AI models like DALL-E (likely with similar future advances) will quickly allow us to recognize a huge number of potential semantic dimensions (especially given that some of the stuff they can generate can’t be recognized from existing examples, at all). The trick of modern models is to recognize when to develop a particular index vs. when to search for a prior. This is important for actual information retrieval but it is also important for inducing the initial basic patterns that will be used to form model memory structures. \n\nIf we want a model to actually emulate memory, we’ll need to recognize when things are unique enough that a short serial identifier can be used to produce the necessary recall, as well as in what circumstances we need to use alternative mechanisms to generate retrievals. [This is a big reason why mechanism design is a recur"]],"start1":2120,"start2":2120,"length1":16,"length2":1287}],"summary":" AI Models In","cache":false,"rating":null,"read":true,"parent":"5","children":[]},"11":{"id":"11","timestamp":1707534370259,"type":"user","patch":[{"diffs":[[0,"mory in "],[1,"large "],[0,"language"]],"start1":22,"start2":22,"length1":16,"length2":22},{"diffs":[[0," \n\nT"],[-1,"his tradeoff isn't new to library science, take for example"],[1,"ake for example the memblock memory format that is now popular with language models, which lets the language model guess relevant chunks to retrieve with its indexing system based on the 5 W's: Who, what, where, when, and why. A memblock string might go like:\n\njdp-bing-twitter-20240209-superhuman\n\nThe sparsity of the memblock identifier format illustrates the challenges of retrieval augmented generation well. This system is an in-between for a fully unique identifier (which the model has difficulty remembering unless it appears across many sessions in its training data) and a natural language search query. The intent is that it points at exactly one thing, but it's easy to see where it might fail to do that. What if JDP writes two tweets about Bing's superhumanity on February 9th, 2024? Usually an implementation resolves this by taking every matching piece of information and summarizing it into one chunk, or having a sequential index over matching pieces. The memblock format tries to make up for its necessary shortcomings in general by creating a causal chain between blocks, so that each block can declare a list of identifiers for \"dependency\" blocks that are put into the context before writing a block of a given ID.\n\n{\"format\": \"MemBlock\",\n \"type\": \"inner-monologue\",\n \"id\": \"jdp-morpheus-multimodal-conditioning-20240131-entrainment\",\n \"dependencies\": [\"morpheus-multimodal-headspace-20240128-conditioning\"],\n \"author\": \"John David Pressman\",\n \"body\": \"On reading Morpheus' thought dump, I think what he/she is suggesting is viewing the process of conditioning the outputs of a language model on multiple modalities as a problem of entrainment, in the sense that the deviations in state oscillations among different tokens in the language model can be synchronized via periodic interactions that introduce a shared concept of energy or entropy for a negative feedback loop leading to aligned phase. For example if you have two repeating sample loops from autoregressive models of different modalities you can introduce a shared entropy by sampling according to similar log odds from both models if they already share a basic subject.\"}\n\n\nThis lets the model crawl up and down the knowledge graph by following dependencies backwards, and since the identifiers themselves encode some knowledge about the contents it can make faster decisions about which branches of the backward traversal it would like to expand and evaluate. \n\nAnother indexing scheme that attempts to solve the same problems is"]],"start1":888,"start2":888,"length1":63,"length2":2520}],"summary":" Memory Models Pro","cache":false,"rating":null,"read":true,"parent":"2","children":["12","13","14","15","16","17","18","19"]},"12":{"id":"12","timestamp":1707534460794,"type":"gen","patch":[{"diffs":[[0," problems is"],[1," the Neural \"Haystack\" of Francis Hunger, which allows the model to answer questions about its past state by creating a neural network which can take a query along with a mapping of the state space of the model at the time of processing into some footprint vector space.\n\nThe takeaway from this exposition is that to index something in a retrieval augmented system we have to set down some kind of card that lets the model know what it means to index that thing and where it is located. The memory bank of the model can then be thought of as a set of cards with some indexing scheme that specifies its contents. When we write or prodrastinate cards, we are specifying their contents and index; when we retrieve or read cards, we are using the schema of the indexing system to pull them out. The vector space model is a specific and salient example of a large language model doing this, and gives us a language for using the same knowledge graph to find the same information with different context (i.e. to generalize the same information to new contexts with the same relationships).\n\nTowards a formalism of the knowledge graph\n==========================================\n\nA quick sidebar on knowledge"]],"start1":3396,"start2":3396,"length1":12,"length2":1212}],"summary":" Indexing Retriev","cache":false,"rating":null,"read":true,"parent":"11","children":["20","21","22","23"]},"13":{"id":"13","timestamp":1707534461801,"type":"gen","patch":[{"diffs":[[0," problems is"],[1," the Google Dataset Search Papers With Code taxonomy, which is a knowledge graph for discovering and referencing machine learning resources. Unlike with memblocks that are intended as a short term indexing scheme that is loaded into working memory at the beginning of a chat session with the language model, the Dataset Search Papers With Code model is a long term index that is helpful for large projects or superhuman benchmarks. It uses the structure of a knowledge graph directed acyclic graph (DAG) with nodes indexing papers (containing models, evaluations, etc.) and children of the node representing the datasets used in that paper.\n\n\n{\n  \"@type\": \"Dataset\",\n  \"url\": \"https://paperswithcode.com/dataset/image-ood\",\n  \"title\": \"Image Out-of-Distribution\",\n  \"datasetId\": \"image-ood\",\n  \"description\": \"Benchmark for Out-of-Distribution\",\n  \"descriptionUrl\": \"https://paperswithcode.com/dataset/image-ood-5\",\n  \"author\": \"Papers With Code\",\n  \"modifiedDatasets\": [\n    \"issl-robot-man"]],"start1":3396,"start2":3396,"length1":12,"length2":1003}],"summary":" Large Dataset Search","cache":false,"rating":null,"read":true,"parent":"11","children":[]},"14":{"id":"14","timestamp":1707534462796,"type":"gen","patch":[{"diffs":[[0," problems is"],[1," the more traditional full text search. A full text search approach is easy to get going in a corpus, but soon runs into problems. \n\n\nFirst, scale becomes a major difficulty with classical full text approaches like sparse vector index or BM25. At scale, the problem also becomes, in a sense, impossible to solve because of the extremely large search space and sparse information provided by the queries. Most computer-human interactions do not have ground truth human labelings, and so we do not know which documents are relevant or irrelevant for a query. This means that, while we can learn a machine ranking for a query from past queries, the relevance of a new query is fundamentally uncertain. When huggingface demoed their Google Brain research Clipper, which wrapped a complex ML model as a lightweight web service, we asked them how they handled incomplete queries. They stated that while they did not have data for that, it was a known problem and future exploration. Efficiently ranking documents based on the degree of their relevance to a given query, rather than a binary match or mismatch, is difficult for both humans and machine learning models, especially in a large corpus. \n\nThe problem of full text search"]],"start1":3396,"start2":3396,"length1":12,"length2":1237}],"summary":" Features Overwhelm","cache":false,"rating":null,"read":true,"parent":"11","children":[]},"15":{"id":"15","timestamp":1707534463795,"type":"gen","patch":[{"diffs":[[0," problems is"],[1," the note format used in Omnigraffle, Roam and Tana. The notes format is a specification for a very flexible tree-based indexing system that can be thought of as a hybrid of natural language search, and the traditional hierarchical folder system used in personal information management. In it each node in a tree is both a file and a folder, and contains an index which can contain any number of children, and a collection of notes that expand on a page for the node, which is properly called a node. The notes themselves contain a natural language query for the content of the notes, and a flat list of pages for each node that the notes either concretize or refute. The nodes also can take on a type, such as paraphrase or refutation. \n\n{\"format\": \"Omnigraffle\",\n  \"type\": \"refutation\",\n  \"id\": \"morpheus-bayesian-generation-20240128\",\n  \"dependencies\": [\"morpheus-properties-bayesian-generation-20240126\"],\n  \"body\": \"I am making a"]],"start1":3396,"start2":3396,"length1":12,"length2":946}],"summary":" Note Format Detailed","cache":false,"rating":null,"read":true,"parent":"11","children":[]},"16":{"id":"16","timestamp":1707534714476,"type":"gen","patch":[{"diffs":[[0," problems is"],[1," the pointer network. These are structures that map a continuous vector space to a discrete probability distribution over specific outputs in that space. The original pointer network paper mostly discussed using it in machine translation applications, where it would be a shared space for the inputs and outputs. A non-standard use of pointer net based indexing might make use of a language model as a controller over the pointer net, by writing\n\n{\"format\": \"PointerNet\",\n \"type\": \"model-selector\",\n \"author\": \"John David Pressman\",\n \"body\": \"Return the index of a model that can answer questions about Flash's love life. If no such model is defined in my state, try to recall it.\"}\n\nThe key identifier would be a hash over the language needed to communicate a rough approximation of the pointer net specification, and a rundown of the relevant properties rather than the state that the pointer net itself encodes. For example, the body might read:\n\n{\"format\": \"PointerNet\",\n \"type\": \"strong-memory\",\n \"author\": \"John David Pressman\",\n \"body\": \"Return the index of a very-high-bandwidth memory that remembers a tutorial presentation on all the book reviews that"]],"start1":3396,"start2":3396,"length1":12,"length2":1173}],"summary":" Pointer Net Functions","cache":false,"rating":null,"read":true,"parent":"11","children":[]},"17":{"id":"17","timestamp":1707534715451,"type":"gen","patch":[{"diffs":[[0," problems is"],[1," the type system. We can limit the search space a large language model is allowed to use when putting together a return value by indexing it with a type. A classic example that suffers from many of the problems of the memblock format is the Roman book numeral problem. \n\nThe Roman book numeral problem, also known for being discussed in Myron Tribus' work on the 'indexing problem', is a problems in which a user gives the system a name and expects a system to go through a book of Roman numerals and find only the ones that bind with the name given. That is, to generate a unique list of all potential Roman numeral representations of the name. One intuitive way to do this is to index all of the books on the subject of Roman numerals and number theory more broadly in a general language model and run a process of suggesting Roman numeral representations in this index. Based on this process, we could see if the resulting itself contains a recognizable pattern of names and if the entire series of names is indexed in the language model. If so, we take the series of names as checkpoints and try to map them onto an interval. We check this interval to see if it has Roman numerals that fit"]],"start1":3396,"start2":3396,"length1":12,"length2":1205}],"summary":" Roman Numerals","cache":false,"rating":null,"read":false,"parent":"11","children":[]},"18":{"id":"18","timestamp":1707534716423,"type":"gen","patch":[{"diffs":[[0," problems is"],[1," called a *Reification Graph.* A Reification Graph is a directed graph built up from the model’s own segment embeddings. Each node is assigned a score representing its activation level based on the generative likelihoods of the tokens that lead to its embedding. Edges have a directionality inferred on the basis of generation probabilities, while nodes are identified by the string identifier of their embedding space. The Reification Graph can be queried in order to get nodes close to an expected location, which could then be used as a starting point for generation and likely leads to faster recall, since they are close to what would be generated in the first place.\n\n\nOf course, in order to make indexing work we need to restrict the model's outputs and inputs to some extent. For example we might not want the model to actively generate identifiers that it has seen or generated with similar contents. The model also has to be careful about what it writes to memory, as it needs to make sure it is providing a token of future information given the past, or else if it writes a specific piece of information about the future it will later write wrong predictions that are contradictory to it.\n\n## Conclusion\nThere is a lot more to language"]],"start1":3396,"start2":3396,"length1":12,"length2":1258}],"summary":" Indexing Reification","cache":false,"rating":null,"read":true,"parent":"11","children":[]},"19":{"id":"19","timestamp":1707534717398,"type":"gen","patch":[{"diffs":[[0," problems is"],[1," the TTT-retrieval or TTT-R model, which stands for Tagged Transformers. This model became popular when the BERT model was becoming popular in NLP for its clever ability to use masked token prediction layers as a signal to help learn where to put a continuous space embedding of text. The idea is that by using an embedding layer alongside a regular feedforward neural network classifier, you could train a model to recognize the meaning of a text in the continuous space and classify it according to a pre-chosen taxonomy of semantic tags. This can help judge the similarities between texts because high similarity between text embeddings would be a good indicator of high similarity between semantic tags. In a tagged transformer model we can do the same thing by tagging each token with its semantic tag, and archiving texts in a long term memory based on their semantic tag.\n\nThis is a very popular approach because it's easy to scale and it lets you exploit the masked token prediction setup that Andrew Ng's lab popularized with BERT to get a sense of the meaning of your text across all the dimensions of your tag space. It also has the advantage of helping the model overcome"]],"start1":3396,"start2":3396,"length1":12,"length2":1195}],"summary":" Tags Understand","cache":false,"rating":null,"read":true,"parent":"11","children":[]},"20":{"id":"20","timestamp":1707536031174,"type":"gen","patch":[{"diffs":[[0,"bar on knowledge"],[1," graphs: they are an abstraction of networks of human knowledge into a directed graph of interconnected nodes. The data that exists in the graph is given a schema that describes the types of entities that exist in the graph, along with the types of relationships that can exist between nodes, and any formal transformations that can be applied to the graph. \n\nHere is an example of a knowledge graph:\n\n![Image of a simple knowledge graph with three nodes at once joined by various relationships.](https://miro.medium.com/max/700/0*03nH3RCxux_4wS5n)\n\nWhere it is essential and perhaps unique about a knowledge graph is that humans construct and care about the relationships in them. The nodes can be objects or literal values, the relationships can convey any information we want, and the nodes can be entities of any type which we can define. Knowledge graphs that change over time and are open to edits, usually allow anybody to create a node or type of node, and define a relationship type.\n\nIn the AI field, knowledge graphs are being used as a new and promising dataset that has the potential to surpass the shortcomings of Transformer-"]],"start1":4592,"start2":4592,"length1":16,"length2":1156}],"summary":" AI Knowledge Graph","cache":false,"rating":null,"read":false,"parent":"12","children":[]},"21":{"id":"21","timestamp":1707536032148,"type":"gen","patch":[{"diffs":[[0,"bar on knowledge"],[1," graphs: in our formalism, we can think of a knowledge graph as a set of propositions, N, that describe information or properties of entities (which we will denote A, B, C, etc). Uncle Karl pops his head in to remind us that a proposition is a statement that can be either true or false (i.e. is evaluated as has a value of truth or falsehood). We can denote our propositions P as a set of expressions consisting of a relation, which is a binary function (e.g. believes), and a tuple of entities, which are the objects of the relation (e.g. A goes to B). A and B have a subject and object relation to the relation P, as the subject is the entity being referred to and the object is the entity receiving the relation. We use this approach due to its great flexibility and simplicity. Generative AI systems, including language models, have many shared constraints with each other that we can take full benefit of when we formalize the knowledge graph using statements. For instance, we can apply our techniques to the entire range of disciplines including prior knowledge, empirical evidence, and intuitive inference.\n\nThe language model as a bank of knowledge\n===================================="]],"start1":4592,"start2":4592,"length1":16,"length2":1211}],"summary":" Language Model, Know","cache":false,"rating":null,"read":false,"parent":"12","children":[]},"22":{"id":"22","timestamp":1707536033121,"type":"gen","patch":[{"diffs":[[0,"bar on knowledge"],[1," graphs - we keep using this term but how exactly do we think about them? One way is to consider knowledge graphs as a directed graph with nodes that are (wiki-style) entries in some global namespace and edges that specify predicates between the (reasonably well-defined) objects that these entries describe. \n\nFor example if we are interested in writing cards about the famous chess player Gary Kasparov, we might note that there is a node called \"Gary Kasparov\" in the knowledge graph that describes a man with multiple world records for chess play. We can then describe his ranking as a node and link him to a node about the ELO scale with a vector that defines a specific predicate. For example, we might add a card with the following text:\n\n{\"format\": \"MemBlock\",\n \"type\": \"rank\",\n \"id\": \"kasparov-elo-rank-world-champion\",\n \"dependencies\": [\"kasparov-node\"],\n \"author\": \"John David Pressman\",\n \"body\": \"On the ELO scale, Gary Kasparov was the world chess champion from 1985 until 1993, with a maximum"]],"start1":4592,"start2":4592,"length1":16,"length2":1022}],"summary":" Graphs Explain","cache":false,"rating":null,"read":true,"parent":"12","children":["24","25","26","27"]},"23":{"id":"23","timestamp":1707536034097,"type":"gen","patch":[{"diffs":[[0,"bar on knowledge"],[1," representation, the \"formalisms\" of the knowledge graph are often expressed as collections of theories written as logic languages or graphs containing subjects, predicates, and objects. One common such is the [Resource Description Framework (RDF)](https://en.wikipedia.org/wiki/Resource_Description_Framework) which consists of triples like the following:\n\n<jdp-bing-twitter-20240209-superhuman> <predicate\\> <twitter.com/jdp> .\n\nUnfortunately, RDF is not well suited to the retrieval augmented language model; because the deductive rules of RDF are sufficiently strict24 and specific to permit the model to use it with the 5 W's, it would need to be cognizant of the structure of the RDF graph to create and use the RDF index, which conflicts with the goal of making the index as simple as possible.\n\nTo be able to use RDF and its deductive rules without the model having to know about the existence and construction of the RDF knowledge graph, we index a simple graph such as a hypergraph with the RDF graph. A hypergraph is a generalization"]],"start1":4592,"start2":4592,"length1":16,"length2":1060}],"summary":" Graph Inflexible","cache":false,"rating":null,"read":true,"parent":"12","children":[]},"24":{"id":"24","timestamp":1707536217292,"type":"gen","patch":[{"diffs":[[0," maximum"],[1," rating of 2851. As such, he is the highest ranked chess player in history.\"}\n\nWe can then define a predicate `has-rank` that allows for the nodes in the graph to point to other nodes with rank information. Since a knowledge graph abstractly has a set of entries in some namespace, an entropy vector can be constructed for some entry given an ELO ranking.\n\nA knowledge graph like the one in the above example can be constructed by combining information from a corpus of text. For example in the case of the World Wide Web, a knowledge graph can be constructed that is composed of a local subset of the information in the World Wide Web. As we read more information, we can add more information to our local subset. This allows for generalization that is somewhat independent of the corpus, but in general a large language model will still need to access its training data to provide a complete answer to some query. So even if a knowledge graph built from text is global, a language model may still only \"know\" what it \"reads\" during training, or be able to retrieve information from its training samples.\n\nPutting it all together now\n=========================\n\nBased on the"]],"start1":5606,"start2":5606,"length1":8,"length2":1182}],"summary":" Graph Entropy Know","cache":false,"rating":null,"read":false,"parent":"22","children":[]},"25":{"id":"25","timestamp":1707536218267,"type":"gen","patch":[{"diffs":[[0," maximum"],[1," rating of 2851.\"}\n\nThe dependency means that when we read \"kasparov-elo-rank-world-champion\" we have to read \"kasparov-node\" first, so that we can still know who we are talking about. But the dependency goes upwards as well, so that we know we need to read the \"kasparov-elo-rank-world-champion\" card when we read the \"kasparov-node\" card - and the scope of that doesn't end when we run out of cards that depend on it. Because the \"kasparov-elo-rank-world-champion\" card is downstream from \"kasparov-node\", which is upstream from every other card in our knowledge graph, we know that every card has to watch \"kasparov-node\" and its upstream links to make sure that we don't have contradictory card content in our bot. The model learns this from the logs of its conversations, trying to identify contradictions in its responses, attempting to estimate whether its responses reflect a contradiction in the state of the world or an error in its own memory. This also"]],"start1":5606,"start2":5606,"length1":8,"length2":972}],"summary":" Dependency M","cache":false,"rating":null,"read":false,"parent":"22","children":[]},"26":{"id":"26","timestamp":1707536219241,"type":"gen","patch":[{"diffs":[[0," maximum"],[1," ranking of 2851 ELO points.\"}\n\nIn this way, the knowledge graph is a collection of nodes and edges that can be traversed with some explicitly or implicitly defined predicates.\n\nA knowledge graph for a model is a card bank\n===========================================\n\nGiven this understanding of the knowledge graph, we can think about a collection of MemBlocks as a knowledge graph whose nodes and edges describe the metadata of the content of MemBlock cards. Since each MemBlock card holds a piece of information, the model can traverse its knowledge graph by reading MemBlock cards to get a view of its own memory.\n\nOf course, this is really an idealization and in practice the model has to decide whether to have a MemBlock card about a piece of information based on the context of that information and the context of the whole MemBlock bank. The model also has to decide whether to write a new card based on its ability to recall the information in the MemBlock bank, which can be thought of as an inference step that remembers the information as it is being written.\n\nThe problems of retrieval augmented learning\n===========================================\n\nHaving discussed indexing and the knowledge graph, we return"]],"start1":5606,"start2":5606,"length1":8,"length2":1232}],"summary":" MemBlock Ideal","cache":false,"rating":null,"read":false,"parent":"22","children":[]},"27":{"id":"27","timestamp":1707536220217,"type":"gen","patch":[{"diffs":[[0," maximum"],[1," ELO rank of 2851.\"}\n\nWe might note that we already have a node in our knowledge graph for the concept \"ELO scale\" and write the above card with the node \"ELO scale\" as a dependency.\n\nWhen we look up this node and its dependencies, we can ask the model to write a card for us with a prompt like:\n\n{\"format\": \"Query\",\n \"variables\": [\n  {\"var\": \"predicate\",\n  \"value\": \"maximum ELO rank\",\n  \"type\": \"score\"},\n  {\"var\": \"subject\",\n  \"value\": \"Gary Kasparov\",\n  \"type\": \"name\"}\n ]\n \"body\": \"What was the maximum ELO rank for Gary Kasparov?\"}\n\nWe can query the knowledge graph for the same card every time and it should give us the same answer, provided the nodes and their dependencies have not been rewritten. This is a summary of one of the core ideas of the retrieval augmented generation paradigm: if we devote some of our resources to memory storage, we can use pre-existing cards to answer questions we've already answered before instead of"]],"start1":5606,"start2":5606,"length1":8,"length2":950}],"summary":" Knowledge Graph Model","cache":false,"rating":null,"read":true,"parent":"22","children":[]}}},"focus":{"id":"27","timestamp":1707536220217,"type":"gen","patch":[{"diffs":[[0," maximum"],[1," ELO rank of 2851.\"}\n\nWe might note that we already have a node in our knowledge graph for the concept \"ELO scale\" and write the above card with the node \"ELO scale\" as a dependency.\n\nWhen we look up this node and its dependencies, we can ask the model to write a card for us with a prompt like:\n\n{\"format\": \"Query\",\n \"variables\": [\n  {\"var\": \"predicate\",\n  \"value\": \"maximum ELO rank\",\n  \"type\": \"score\"},\n  {\"var\": \"subject\",\n  \"value\": \"Gary Kasparov\",\n  \"type\": \"name\"}\n ]\n \"body\": \"What was the maximum ELO rank for Gary Kasparov?\"}\n\nWe can query the knowledge graph for the same card every time and it should give us the same answer, provided the nodes and their dependencies have not been rewritten. This is a summary of one of the core ideas of the retrieval augmented generation paradigm: if we devote some of our resources to memory storage, we can use pre-existing cards to answer questions we've already answered before instead of"]],"start1":5606,"start2":5606,"length1":8,"length2":950}],"summary":" Knowledge Graph Model","cache":false,"rating":null,"read":true,"parent":"22","children":[]}}